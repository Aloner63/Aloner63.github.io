<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Aloner63 的个人博客</title><link>https://Aloner63.github.io</link><description>分享技术，记录生活，探索未知</description><copyright>Aloner63 的个人博客</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg</url><title>avatar</title><link>https://Aloner63.github.io</link></image><lastBuildDate>Fri, 26 Dec 2025 11:18:24 +0000</lastBuildDate><managingEditor>Aloner63 的个人博客</managingEditor><ttl>60</ttl><webMaster>Aloner63 的个人博客</webMaster><item><title>unet++</title><link>https://Aloner63.github.io/post/unet%2B%2B.html</link><description>train.py
```
import argparse
import os
from collections import OrderedDict
from glob import glob

import pandas as pd
import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
import torch.optim as optim
import yaml
from albumentations.augmentations import transforms
from albumentations.core.composition import Compose, OneOf
from sklearn.model_selection import train_test_split
from torch.optim import lr_scheduler
from tqdm import tqdm

import archs
import losses
from dataset import Dataset
from metrics import iou_score
from utils import AverageMeter, str2bool

ARCH_NAMES = archs.__all__
LOSS_NAMES = losses.__all__
LOSS_NAMES.append('BCEWithLogitsLoss')


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--name', default=None,
                        help='model name: (default: arch+timestamp)')
    parser.add_argument('--epochs', default=100, type=int, metavar='N',
                        help='number of total epochs to run')
    parser.add_argument('-b', '--batch_size', default=16, type=int,
                        metavar='N', help='mini-batch size (default: 16)')
    
    # model
    parser.add_argument('--arch', '-a', metavar='ARCH', default='NestedUNet',
                        choices=ARCH_NAMES,
                        help='model architecture: ' +
                        ' | '.join(ARCH_NAMES) +
                        ' (default: NestedUNet)')
    parser.add_argument('--deep_supervision', default=False, type=str2bool)
    parser.add_argument('--input_channels', default=3, type=int,
                        help='input channels')
    parser.add_argument('--num_classes', default=1, type=int,
                        help='number of classes')
    parser.add_argument('--input_w', default=96, type=int,
                        help='image width')
    parser.add_argument('--input_h', default=96, type=int,
                        help='image height')
    
    # loss
    parser.add_argument('--loss', default='BCEDiceLoss',
                        choices=LOSS_NAMES,
                        help='loss: ' +
                        ' | '.join(LOSS_NAMES) +
                        ' (default: BCEDiceLoss)')
    
    # dataset
    parser.add_argument('--dataset', default='dsb2018_96',
                        help='dataset name')
    parser.add_argument('--img_ext', default='.png',
                        help='image file extension')
    parser.add_argument('--mask_ext', default='.png',
                        help='mask file extension')

    # optimizer
    parser.add_argument('--optimizer', default='SGD',
                        choices=['Adam', 'SGD'],
                        help='loss: ' +
                        ' | '.join(['Adam', 'SGD']) +
                        ' (default: Adam)')
    parser.add_argument('--lr', '--learning_rate', default=1e-3, type=float,
                        metavar='LR', help='initial learning rate')
    parser.add_argument('--momentum', default=0.9, type=float,
                        help='momentum')
    parser.add_argument('--weight_decay', default=1e-4, type=float,
                        help='weight decay')
    parser.add_argument('--nesterov', default=False, type=str2bool,
                        help='nesterov')

    # scheduler
    parser.add_argument('--scheduler', default='CosineAnnealingLR',
                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])
    parser.add_argument('--min_lr', default=1e-5, type=float,
                        help='minimum learning rate')
    parser.add_argument('--factor', default=0.1, type=float)
    parser.add_argument('--patience', default=2, type=int)
    parser.add_argument('--milestones', default='1,2', type=str)
    parser.add_argument('--gamma', default=2/3, type=float)
    parser.add_argument('--early_stopping', default=-1, type=int,
                        metavar='N', help='early stopping (default: -1)')
    
    parser.add_argument('--num_workers', default=4, type=int)

    config = parser.parse_args()

    return config


def train(config, train_loader, model, criterion, optimizer):
    avg_meters = {'loss': AverageMeter(),
                  'iou': AverageMeter()}

    model.train()

    pbar = tqdm(total=len(train_loader))
    for input, target, _ in train_loader:
        input = input.cuda()
        target = target.cuda()

        # compute output
        if config['deep_supervision']:
            outputs = model(input)
            loss = 0
            for output in outputs:
                loss += criterion(output, target)
            loss /= len(outputs)
            iou = iou_score(outputs[-1], target)
        else:
            output = model(input)
            loss = criterion(output, target)
            iou = iou_score(output, target)

        # compute gradient and do optimizing step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        avg_meters['loss'].update(loss.item(), input.size(0))
        avg_meters['iou'].update(iou, input.size(0))

        postfix = OrderedDict([
            ('loss', avg_meters['loss'].avg),
            ('iou', avg_meters['iou'].avg),
        ])
        pbar.set_postfix(postfix)
        pbar.update(1)
    pbar.close()

    return OrderedDict([('loss', avg_meters['loss'].avg),
                        ('iou', avg_meters['iou'].avg)])


def validate(config, val_loader, model, criterion):
    avg_meters = {'loss': AverageMeter(),
                  'iou': AverageMeter()}

    # switch to evaluate mode
    model.eval()

    with torch.no_grad():
        pbar = tqdm(total=len(val_loader))
        for input, target, _ in val_loader:
            input = input.cuda()
            target = target.cuda()

            # compute output
            if config['deep_supervision']:
                outputs = model(input)
                loss = 0
                for output in outputs:
                    loss += criterion(output, target)
                loss /= len(outputs)
                iou = iou_score(outputs[-1], target)
            else:
                output = model(input)
                loss = criterion(output, target)
                iou = iou_score(output, target)

            avg_meters['loss'].update(loss.item(), input.size(0))
            avg_meters['iou'].update(iou, input.size(0))

            postfix = OrderedDict([
                ('loss', avg_meters['loss'].avg),
                ('iou', avg_meters['iou'].avg),
            ])
            pbar.set_postfix(postfix)
            pbar.update(1)
        pbar.close()

    return OrderedDict([('loss', avg_meters['loss'].avg),
                        ('iou', avg_meters['iou'].avg)])


def main():
    config = vars(parse_args())

    if config['name'] is None:
        if config['deep_supervision']:
            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])
        else:
            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])
    os.makedirs('models/%s' % config['name'], exist_ok=True)

    print('-' * 20)
    for key in config:
        print('%s: %s' % (key, config[key]))
    print('-' * 20)

    with open('models/%s/config.yml' % config['name'], 'w') as f:
        yaml.dump(config, f)

    # define loss function (criterion)
    if config['loss'] == 'BCEWithLogitsLoss':
        criterion = nn.BCEWithLogitsLoss().cuda()
    else:
        criterion = losses.__dict__[config['loss']]().cuda()

    cudnn.benchmark = True

    # create model
    print('=&gt; creating model %s' % config['arch'])
    model = archs.__dict__[config['arch']](config['num_classes'],
                                           config['input_channels'],
                                           config['deep_supervision'])

    model = model.cuda()

    params = filter(lambda p: p.requires_grad, model.parameters())
    if config['optimizer'] == 'Adam':
        optimizer = optim.Adam(
            params, lr=config['lr'], weight_decay=config['weight_decay'])
    elif config['optimizer'] == 'SGD':
        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],
                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])
    else:
        raise NotImplementedError

    if config['scheduler'] == 'CosineAnnealingLR':
        scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])
    elif config['scheduler'] == 'ReduceLROnPlateau':
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],
                                                   verbose=1, min_lr=config['min_lr'])
    elif config['scheduler'] == 'MultiStepLR':
        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')], gamma=config['gamma'])
    elif config['scheduler'] == 'ConstantLR':
        scheduler = None
    else:
        raise NotImplementedError

    # Data loading code
    img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))
    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]

    train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)

    train_transform = Compose([
        transforms.RandomRotate90(),
        transforms.Flip(),
        OneOf([
            transforms.HueSaturationValue(),
            transforms.RandomBrightness(),
            transforms.RandomContrast(),
        ], p=1),
        transforms.Resize(config['input_h'], config['input_w']),
        transforms.Normalize(),
    ])

    val_transform = Compose([
        transforms.Resize(config['input_h'], config['input_w']),
        transforms.Normalize(),
    ])

    train_dataset = Dataset(
        img_ids=train_img_ids,
        img_dir=os.path.join('inputs', config['dataset'], 'images'),
        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),
        img_ext=config['img_ext'],
        mask_ext=config['mask_ext'],
        num_classes=config['num_classes'],
        transform=train_transform)
    val_dataset = Dataset(
        img_ids=val_img_ids,
        img_dir=os.path.join('inputs', config['dataset'], 'images'),
        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),
        img_ext=config['img_ext'],
        mask_ext=config['mask_ext'],
        num_classes=config['num_classes'],
        transform=val_transform)

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        drop_last=True)
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        drop_last=False)

    log = OrderedDict([
        ('epoch', []),
        ('lr', []),
        ('loss', []),
        ('iou', []),
        ('val_loss', []),
        ('val_iou', []),
    ])

    best_iou = 0
    trigger = 0
    for epoch in range(config['epochs']):
        print('Epoch [%d/%d]' % (epoch, config['epochs']))

        # train for one epoch
        train_log = train(config, train_loader, model, criterion, optimizer)
        # evaluate on validation set
        val_log = validate(config, val_loader, model, criterion)

        if config['scheduler'] == 'CosineAnnealingLR':
            scheduler.step()
        elif config['scheduler'] == 'ReduceLROnPlateau':
            scheduler.step(val_log['loss'])

        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'
              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))

        log['epoch'].append(epoch)
        log['lr'].append(config['lr'])
        log['loss'].append(train_log['loss'])
        log['iou'].append(train_log['iou'])
        log['val_loss'].append(val_log['loss'])
        log['val_iou'].append(val_log['iou'])

        pd.DataFrame(log).to_csv('models/%s/log.csv' %
                                 config['name'], index=False)

        trigger += 1

        if val_log['iou'] &gt; best_iou:
            torch.save(model.state_dict(), 'models/%s/model.pth' %
                       config['name'])
            best_iou = val_log['iou']
            print('=&gt; saved best model')
            trigger = 0

        # early stopping
        if config['early_stopping'] &gt;= 0 and trigger &gt;= config['early_stopping']:
            print('=&gt; early stopping')
            break

        torch.cuda.empty_cache()


if __name__ == '__main__':
    main()

```


archs.py
```
import torch
from torch import nn

__all__ = ['UNet', 'NestedUNet']


class VGGBlock(nn.Module):
    def __init__(self, in_channels, middle_channels, out_channels):
        super().__init__()
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(middle_channels)
        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        return out


class UNet(nn.Module):
    def __init__(self, num_classes, input_channels=3, **kwargs):
        super().__init__()

        nb_filter = [32, 64, 128, 256, 512]

        self.pool = nn.MaxPool2d(2, 2)
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])
        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])
        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])
        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])
        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])

        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])
        self.conv2_2 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])
        self.conv1_3 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])
        self.conv0_4 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])

        self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)


    def forward(self, input):
        x0_0 = self.conv0_0(input)
        x1_0 = self.conv1_0(self.pool(x0_0))
        x2_0 = self.conv2_0(self.pool(x1_0))
        x3_0 = self.conv3_0(self.pool(x2_0))
        x4_0 = self.conv4_0(self.pool(x3_0))

        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))
        x2_2 = self.conv2_2(torch.cat([x2_0, self.up(x3_1)], 1))
        x1_3 = self.conv1_3(torch.cat([x1_0, self.up(x2_2)], 1))
        x0_4 = self.conv0_4(torch.cat([x0_0, self.up(x1_3)], 1))

        output = self.final(x0_4)
        return output


class NestedUNet(nn.Module):
    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):
        super().__init__()

        nb_filter = [32, 64, 128, 256, 512]

        self.deep_supervision = deep_supervision

        self.pool = nn.MaxPool2d(2, 2)
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])
        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])
        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])
        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])
        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])

        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])
        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])
        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])
        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])

        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])
        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])
        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])

        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])
        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])

        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])

        if self.deep_supervision:
            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)
            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)
            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)
            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)
        else:
            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)


    def forward(self, input):
        x0_0 = self.conv0_0(input)
        x1_0 = self.conv1_0(self.pool(x0_0))
        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))

        x2_0 = self.conv2_0(self.pool(x1_0))
        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))

        x3_0 = self.conv3_0(self.pool(x2_0))
        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))
        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))

        x4_0 = self.conv4_0(self.pool(x3_0))
        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))
        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))
        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))
        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))

        if self.deep_supervision:
            output1 = self.final1(x0_1)
            output2 = self.final2(x0_2)
            output3 = self.final3(x0_3)
            output4 = self.final4(x0_4)
            return [output1, output2, output3, output4]

        else:
            output = self.final(x0_4)
            return output

```

val.py
```
import argparse
import os
from glob import glob

import cv2
import torch
import torch.backends.cudnn as cudnn
import yaml
from albumentations.augmentations import transforms
from albumentations.core.composition import Compose
from sklearn.model_selection import train_test_split
from tqdm import tqdm

import archs
from dataset import Dataset
from metrics import iou_score
from utils import AverageMeter


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--name', default=None,
                        help='model name')

    args = parser.parse_args()

    return args


def main():
    args = parse_args()

    with open('models/%s/config.yml' % args.name, 'r') as f:
        config = yaml.load(f, Loader=yaml.FullLoader)

    print('-'*20)
    for key in config.keys():
        print('%s: %s' % (key, str(config[key])))
    print('-'*20)

    cudnn.benchmark = True

    # create model
    print('=&gt; creating model %s' % config['arch'])
    model = archs.__dict__[config['arch']](config['num_classes'],
                                           config['input_channels'],
                                           config['deep_supervision'])

    model = model.cuda()

    # Data loading code
    img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))
    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]

    _, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)

    model.load_state_dict(torch.load('models/%s/model.pth' %
                                     config['name']))
    model.eval()

    val_transform = Compose([
        transforms.Resize(config['input_h'], config['input_w']),
        transforms.Normalize(),
    ])

    val_dataset = Dataset(
        img_ids=val_img_ids,
        img_dir=os.path.join('inputs', config['dataset'], 'images'),
        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),
        img_ext=config['img_ext'],
        mask_ext=config['mask_ext'],
        num_classes=config['num_classes'],
        transform=val_transform)
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        drop_last=False)

    avg_meter = AverageMeter()

    for c in range(config['num_classes']):
        os.makedirs(os.path.join('outputs', config['name'], str(c)), exist_ok=True)
    with torch.no_grad():
        for input, target, meta in tqdm(val_loader, total=len(val_loader)):
            input = input.cuda()
            target = target.cuda()

            # compute output
            if config['deep_supervision']:
                output = model(input)[-1]
            else:
                output = model(input)

            iou = iou_score(output, target)
            avg_meter.update(iou, input.size(0))

            output = torch.sigmoid(output).cpu().numpy()

            for i in range(len(output)):
                for c in range(config['num_classes']):
                    cv2.imwrite(os.path.join('outputs', config['name'], str(c), meta['img_id'][i] + '.jpg'),
                                (output[i, c] * 255).astype('uint8'))

    print('IoU: %.4f' % avg_meter.avg)

    torch.cuda.empty_cache()


if __name__ == '__main__':
    main()

```

metrics.py
```
import numpy as np
import torch
import torch.nn.functional as F


def iou_score(output, target):
    smooth = 1e-5

    if torch.is_tensor(output):
        output = torch.sigmoid(output).data.cpu().numpy()
    if torch.is_tensor(target):
        target = target.data.cpu().numpy()
    output_ = output &gt; 0.5
    target_ = target &gt; 0.5
    intersection = (output_ &amp; target_).sum()
    union = (output_ | target_).sum()

    return (intersection + smooth) / (union + smooth)


def dice_coef(output, target):
    smooth = 1e-5

    output = torch.sigmoid(output).view(-1).data.cpu().numpy()
    target = target.view(-1).data.cpu().numpy()
    intersection = (output * target).sum()

    return (2. * intersection + smooth) / \
        (output.sum() + target.sum() + smooth)

```
utils.py
```
import argparse


def str2bool(v):
    if v.lower() in ['true', 1]:
        return True
    elif v.lower() in ['false', 0]:
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def count_params(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


class AverageMeter(object):
    '''Computes and stores the average and current value'''

    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

```

losses.py
```
import torch
import torch.nn as nn
import torch.nn.functional as F

try:
    from LovaszSoftmax.pytorch.lovasz_losses import lovasz_hinge
except ImportError:
    pass

__all__ = ['BCEDiceLoss', 'LovaszHingeLoss']


class BCEDiceLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, target):
        bce = F.binary_cross_entropy_with_logits(input, target)
        smooth = 1e-5
        input = torch.sigmoid(input)
        num = target.size(0)
        input = input.view(num, -1)
        target = target.view(num, -1)
        intersection = (input * target)
        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)
        dice = 1 - dice.sum() / num
        return 0.5 * bce + dice


class LovaszHingeLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input, target):
        input = input.squeeze(1)
        target = target.squeeze(1)
        loss = lovasz_hinge(input, target, per_image=True)

        return loss

```
dataset.py
```
import os

import cv2
import numpy as np
import torch
import torch.utils.data


class Dataset(torch.utils.data.Dataset):
    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, num_classes, transform=None):
        '''
        Args:
            img_ids (list): Image ids.
            img_dir: Image file directory.
            mask_dir: Mask file directory.
            img_ext (str): Image file extension.
            mask_ext (str): Mask file extension.
            num_classes (int): Number of classes.
            transform (Compose, optional): Compose transforms of albumentations. Defaults to None.
        
        Note:
            Make sure to put the files as the following structure:
            &lt;dataset name&gt;
            ├── images
            |   ├── 0a7e06.jpg
            │   ├── 0aab0a.jpg
            │   ├── 0b1761.jpg
            │   ├── ...
            |
            └── masks
                ├── 0
                |   ├── 0a7e06.png
                |   ├── 0aab0a.png
                |   ├── 0b1761.png
                |   ├── ...
                |
                ├── 1
                |   ├── 0a7e06.png
                |   ├── 0aab0a.png
                |   ├── 0b1761.png
                |   ├── ...
                ...
        '''
        self.img_ids = img_ids
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.img_ext = img_ext
        self.mask_ext = mask_ext
        self.num_classes = num_classes
        self.transform = transform

    def __len__(self):
        return len(self.img_ids)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        
        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))

        mask = []
        for i in range(self.num_classes):
            mask.append(cv2.imread(os.path.join(self.mask_dir, str(i),
                        img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])
        mask = np.dstack(mask)

        if self.transform is not None:
            augmented = self.transform(image=img, mask=mask)
            img = augmented['image']
            mask = augmented['mask']
        
        img = img.astype('float32') / 255
        img = img.transpose(2, 0, 1)
        mask = mask.astype('float32') / 255
        mask = mask.transpose(2, 0, 1)
        
        return img, mask, {'img_id': img_id}

```

preprocess_dsb2018.py
```
import os
from glob import glob

import cv2
import numpy as np
from tqdm import tqdm


def main():
    img_size = 96

    paths = glob('inputs/data-science-bowl-2018/stage1_train/*')

    os.makedirs('inputs/dsb2018_%d/images' % img_size, exist_ok=True)
    os.makedirs('inputs/dsb2018_%d/masks/0' % img_size, exist_ok=True)

    for i in tqdm(range(len(paths))):
        path = paths[i]
        img = cv2.imread(os.path.join(path, 'images',
                         os.path.basename(path) + '.png'))
        mask = np.zeros((img.shape[0], img.shape[1]))
        for mask_path in glob(os.path.join(path, 'masks', '*')):
            mask_ = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) &gt; 127
            mask[mask_] = 1
        if len(img.shape) == 2:
            img = np.tile(img[..., None], (1, 1, 3))
        if img.shape[2] == 4:
            img = img[..., :3]
        img = cv2.resize(img, (img_size, img_size))
        mask = cv2.resize(mask, (img_size, img_size))
        cv2.imwrite(os.path.join('inputs/dsb2018_%d/images' % img_size,
                    os.path.basename(path) + '.png'), img)
        cv2.imwrite(os.path.join('inputs/dsb2018_%d/masks/0' % img_size,
                    os.path.basename(path) + '.png'), (mask * 255).astype('uint8'))


if __name__ == '__main__':
    main()

```
。</description><guid isPermaLink="true">https://Aloner63.github.io/post/unet%2B%2B.html</guid><pubDate>Fri, 26 Dec 2025 11:17:56 +0000</pubDate></item><item><title>kc-unet++源码</title><link>https://Aloner63.github.io/post/kc-unet%2B%2B-yuan-ma.html</link><description>train.py
```
import argparse
import os
from collections import OrderedDict
from glob import glob

import pandas as pd
import torch
import torch.backends.cudnn as cudnn
import torch.nn as nn
import torch.optim as optim
import yaml
import albumentations as A
from albumentations.core.composition import Compose, OneOf
from sklearn.model_selection import train_test_split
from torch.optim import lr_scheduler
from tqdm import tqdm

import archs
import losses
from dataset import Dataset
from metrics import iou_score, dice_coef, precision_score, recall_score
from utils import AverageMeter, str2bool

ARCH_NAMES = archs.__all__
LOSS_NAMES = losses.__all__
LOSS_NAMES.append('BCEWithLogitsLoss')


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('--name', default=None,
                        help='model name: (default: arch+timestamp)')
    parser.add_argument('--epochs', default=150, type=int, metavar='N',
                        help='number of total epochs to run')
    parser.add_argument('-b', '--batch_size', default=8, type=int,
                        metavar='N', help='mini-batch size (default: 16)')

    # model
    parser.add_argument('--arch', '-a', metavar='ARCH', default='UKAN_NestedUNet',
                        choices=ARCH_NAMES,
                        help='model architecture: ' +
                             ' | '.join(ARCH_NAMES) +
                             ' (default: NestedUNet)')
    parser.add_argument('--deep_supervision', default=False, type=str2bool)
    parser.add_argument('--input_channels', default=3, type=int,
                        help='input channels')
    parser.add_argument('--num_classes', default=1, type=int,
                        help='number of classes')
    parser.add_argument('--input_w', default=512, type=int,
                        help='image width')
    parser.add_argument('--input_h', default=512, type=int,
                        help='image height')

    # loss
    parser.add_argument('--loss', default='BCEDiceLoss',
                        choices=LOSS_NAMES,
                        help='loss: ' +
                             ' | '.join(LOSS_NAMES) +
                             ' (default: BCEDiceLoss)')

    # dataset
    parser.add_argument('--dataset', default='dsb2018_96_1000',
                        help='dataset name')
    parser.add_argument('--img_ext', default='.jpg',
                        help='image file extension')
    parser.add_argument('--mask_ext', default='.png',
                        help='mask file extension')

    # optimizer
    parser.add_argument('--optimizer', default='SGD',
                        choices=['Adam', 'SGD'],
                        help='loss: ' +
                             ' | '.join(['Adam', 'SGD']) +
                             ' (default: Adam)')
    parser.add_argument('--lr', '--learning_rate', default=1e-3, type=float,
                        metavar='LR', help='initial learning rate')
    parser.add_argument('--momentum', default=0.9, type=float,
                        help='momentum')
    parser.add_argument('--weight_decay', default=1e-4, type=float,
                        help='weight decay')
    parser.add_argument('--nesterov', default=False, type=str2bool,
                        help='nesterov')

    # scheduler
    parser.add_argument('--scheduler', default='CosineAnnealingLR',
                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])
    parser.add_argument('--min_lr', default=1e-5, type=float,
                        help='minimum learning rate')
    parser.add_argument('--factor', default=0.1, type=float)
    parser.add_argument('--patience', default=2, type=int)
    parser.add_argument('--milestones', default='1,2', type=str)
    parser.add_argument('--gamma', default=2 / 3, type=float)
    parser.add_argument('--early_stopping', default=-1, type=int,
                        metavar='N', help='early stopping (default: -1)')

    parser.add_argument('--num_workers', default=4, type=int)

    config = parser.parse_args()

    return config


def train(config, train_loader, model, criterion, optimizer):
    avg_meters = {'loss': AverageMeter(),
                  'iou': AverageMeter(),
                  'dice': AverageMeter(),
                  'precision': AverageMeter(),
                  'recall': AverageMeter()}

    model.train()

    pbar = tqdm(total=len(train_loader))
    for input, target, _ in train_loader:
        input = input.cuda()
        target = target.cuda()

        # compute output
        if config['deep_supervision']:
            outputs = model(input)
            loss = 0
            for output in outputs:
                loss += criterion(output, target)
            loss /= len(outputs)
            output = outputs[-1]
        else:
            output = model(input)
            loss = criterion(output, target)

        # compute metrics
        iou = iou_score(output, target)
        dice = dice_coef(output, target)
        precision = precision_score(output, target)
        recall = recall_score(output, target)

        # compute gradient and do optimizing step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        avg_meters['loss'].update(loss.item(), input.size(0))
        avg_meters['iou'].update(iou, input.size(0))
        avg_meters['dice'].update(dice, input.size(0))
        avg_meters['precision'].update(precision, input.size(0))
        avg_meters['recall'].update(recall, input.size(0))

        postfix = OrderedDict([
            ('loss', avg_meters['loss'].avg),
            ('iou', avg_meters['iou'].avg),
            ('dice', avg_meters['dice'].avg),
            ('precision', avg_meters['precision'].avg),
            ('recall', avg_meters['recall'].avg),
        ])
        pbar.set_postfix(postfix)
        pbar.update(1)
    pbar.close()

    return OrderedDict([('loss', avg_meters['loss'].avg),
                        ('iou', avg_meters['iou'].avg),
                        ('dice', avg_meters['dice'].avg),
                        ('precision', avg_meters['precision'].avg),
                        ('recall', avg_meters['recall'].avg)])


def validate(config, val_loader, model, criterion):
    avg_meters = {'loss': AverageMeter(),
                  'iou': AverageMeter(),
                  'dice': AverageMeter(),
                  'precision': AverageMeter(),
                  'recall': AverageMeter()}

    # switch to evaluate mode
    model.eval()

    with torch.no_grad():
        pbar = tqdm(total=len(val_loader))
        for input, target, _ in val_loader:
            input = input.cuda()
            target = target.cuda()

            # compute output
            if config['deep_supervision']:
                outputs = model(input)
                loss = 0
                for output in outputs:
                    loss += criterion(output, target)
                loss /= len(outputs)
                output = outputs[-1]
            else:
                output = model(input)
                loss = criterion(output, target)

            # compute metrics
            iou = iou_score(output, target)
            dice = dice_coef(output, target)
            precision = precision_score(output, target)
            recall = recall_score(output, target)

            avg_meters['loss'].update(loss.item(), input.size(0))
            avg_meters['iou'].update(iou, input.size(0))
            avg_meters['dice'].update(dice, input.size(0))
            avg_meters['precision'].update(precision, input.size(0))
            avg_meters['recall'].update(recall, input.size(0))

            postfix = OrderedDict([
                ('loss', avg_meters['loss'].avg),
                ('iou', avg_meters['iou'].avg),
                ('dice', avg_meters['dice'].avg),
                ('precision', avg_meters['precision'].avg),
                ('recall', avg_meters['recall'].avg),
            ])
            pbar.set_postfix(postfix)
            pbar.update(1)
        pbar.close()

    return OrderedDict([('loss', avg_meters['loss'].avg),
                        ('iou', avg_meters['iou'].avg),
                        ('dice', avg_meters['dice'].avg),
                        ('precision', avg_meters['precision'].avg),
                        ('recall', avg_meters['recall'].avg)])


def main():
    config = vars(parse_args())

    if config['name'] is None:
        if config['deep_supervision']:
            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])
        else:
            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])
    os.makedirs('models/%s' % config['name'], exist_ok=True)

    print('-' * 20)
    for key in config:
        print('%s: %s' % (key, config[key]))
    print('-' * 20)

    with open('models/%s/config.yml' % config['name'], 'w') as f:
        yaml.dump(config, f)

    # define loss function (criterion)
    if config['loss'] == 'BCEWithLogitsLoss':
        criterion = nn.BCEWithLogitsLoss().cuda()
    else:
        criterion = losses.__dict__[config['loss']]().cuda()

    cudnn.benchmark = True

    # create model
    print('=&gt; creating model %s' % config['arch'])
    model = archs.__dict__[config['arch']](config['num_classes'],
                                           config['input_channels'],
                                           config['deep_supervision'])

    # Enable multi-GPU support with DataParallel
    if torch.cuda.device_count() &gt; 1:
        print(f'=&gt; Using {torch.cuda.device_count()} GPUs!')
        model = nn.DataParallel(model)
    model = model.cuda()

    params = filter(lambda p: p.requires_grad, model.parameters())
    if config['optimizer'] == 'Adam':
        optimizer = optim.Adam(
            params, lr=config['lr'], weight_decay=config['weight_decay'])
    elif config['optimizer'] == 'SGD':
        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],
                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])
    else:
        raise NotImplementedError

    if config['scheduler'] == 'CosineAnnealingLR':
        scheduler = lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])
    elif config['scheduler'] == 'ReduceLROnPlateau':
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],
                                                   verbose=1, min_lr=config['min_lr'])
    elif config['scheduler'] == 'MultiStepLR':
        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')],
                                             gamma=config['gamma'])
    elif config['scheduler'] == 'ConstantLR':
        scheduler = None
    else:
        raise NotImplementedError

    # Data loading code
    img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))
    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]

    train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)

    train_transform = Compose([
        A.RandomRotate90(),
        A.HorizontalFlip(p=0.5),  # 50% 概率水平翻转
        A.VerticalFlip(p=0.5),  # 50% 概率垂直翻转
        OneOf([
            A.HueSaturationValue(),
            A.RandomBrightnessContrast(),
        ], p=1),
        A.Resize(config['input_h'], config['input_w']),
        A.Normalize(),
    ])

    val_transform = Compose([
        A.Resize(config['input_h'], config['input_w']),
        A.Normalize(),
    ])

    train_dataset = Dataset(
        img_ids=train_img_ids,
        img_dir=os.path.join('inputs', config['dataset'], 'images'),
        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),
        img_ext=config['img_ext'],
        mask_ext=config['mask_ext'],
        num_classes=config['num_classes'],
        transform=train_transform)
    val_dataset = Dataset(
        img_ids=val_img_ids,
        img_dir=os.path.join('inputs', config['dataset'], 'images'),
        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),
        img_ext=config['img_ext'],
        mask_ext=config['mask_ext'],
        num_classes=config['num_classes'],
        transform=val_transform)

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=config['num_workers'],
        drop_last=True)
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        drop_last=False)

    # 创建CSV文件
    df = pd.DataFrame(columns=['epoch', 'loss', 'iou', 'dice', 'precision', 'recall',
                              'val_loss', 'val_iou', 'val_dice', 'val_precision', 'val_recall'])
    df.to_csv('models/%s/log.csv' % config['name'], index=False)

    best_iou = 0
    best_epoch = 0
    for epoch in range(config['epochs']):
        print('\nEpoch [%d/%d]' % (epoch, config['epochs']))

        # train for one epoch
        train_log = train(config, train_loader, model, criterion, optimizer)
        val_log = validate(config, val_loader, model, criterion)

        if config['scheduler'] == 'CosineAnnealingLR':
            scheduler.step()
        elif config['scheduler'] == 'ReduceLROnPlateau':
            scheduler.step(val_log['loss'])

        print('loss %.4f - iou %.4f - dice %.4f - precision %.4f - recall %.4f - val_loss %.4f - val_iou %.4f - val_dice %.4f - val_precision %.4f - val_recall %.4f'
              % (train_log['loss'], train_log['iou'], train_log['dice'], train_log['precision'], train_log['recall'],
                 val_log['loss'], val_log['iou'], val_log['dice'], val_log['precision'], val_log['recall']))

        df = pd.DataFrame([[epoch, train_log['loss'], train_log['iou'], train_log['dice'], train_log['precision'], train_log['recall'],
                          val_log['loss'], val_log['iou'], val_log['dice'], val_log['precision'], val_log['recall']]],
                         columns=['epoch', 'loss', 'iou', 'dice', 'precision', 'recall',
                                 'val_loss', 'val_iou', 'val_dice', 'val_precision', 'val_recall'])
        df.to_csv('models/%s/log.csv' % config['name'], mode='a', header=False, index=False)

        if val_log['iou'] &gt; best_iou:
            print('=&gt; saved best model')
            best_iou = val_log['iou']
            best_epoch = epoch
            torch.save(model.state_dict(), 'models/%s/model.pth' % config['name'])

    print('=&gt; Best IoU: %.4f at epoch %d' % (best_iou, best_epoch))


if __name__ == '__main__':
    main()
```

archs.py
```

import torch
from torch import nn
import torch.nn.functional as F
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
from kan import KANLinear
from model.attention.CBAM import CBAMBlock  # 导入CBAM模块

__all__ = ['UKAN_NestedUNet']

class KANLayer(nn.Module):
    '''
    KAN层实现，基于Kolmogorov-Arnold网络
    这是一个特殊的神经网络层，使用样条函数来增强网络的表达能力
    '''
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        # 网格大小，控制样条函数的复杂度
        grid_size = 3
        # 样条函数的阶数
        spline_order = 2
        # 噪声缩放因子，用于初始化
        scale_noise = 0.1
        # 基础缩放因子
        scale_base = 1.0
        # 样条缩放因子
        scale_spline = 1.0
        # 基础激活函数
        base_activation = nn.SiLU
        # 网格epsilon值，防止网格点过于接近
        grid_eps = 0.02
        # 网格范围
        grid_range = [-1, 1]

        # 第一个KAN线性层，将输入特征映射到隐藏特征
        self.fc1 = KANLinear(
            in_features,
            hidden_features,
            grid_size=grid_size,
            spline_order=spline_order,
            scale_noise=scale_noise,
            scale_base=scale_base,
            scale_spline=scale_spline,
            base_activation=base_activation,
            grid_eps=grid_eps,
            grid_range=grid_range,
        )
        # 第二个KAN线性层，将隐藏特征映射到输出特征
        self.fc2 = KANLinear(
            hidden_features,
            out_features,
            grid_size=grid_size,
            spline_order=spline_order,
            scale_noise=scale_noise,
            scale_base=scale_base,
            scale_spline=scale_spline,
            base_activation=base_activation,
            grid_eps=grid_eps,
            grid_range=grid_range,
        )
        # 深度可分离卷积，用于捕获空间信息
        self.dwconv = nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features)
        # 批归一化层
        self.bn = nn.BatchNorm2d(hidden_features)
        # ReLU激活函数
        self.relu = nn.ReLU()
        # Dropout层，用于正则化
        self.drop = nn.Dropout(drop)

    def forward(self, x, H, W):
        '''
        前向传播函数
        x: 输入特征 [B, N, C]
        H, W: 特征图的高和宽
        '''
        B, N, C = x.shape
        # 应用第一个KAN线性层
        x = self.fc1(x.reshape(B * N, C)).reshape(B, N, -1)
        # 重塑为图像格式以应用卷积
        x = x.transpose(1, 2).view(B, -1, H, W)
        # 应用深度可分离卷积、批归一化和ReLU
        x = self.relu(self.bn(self.dwconv(x)))
        # 重塑回序列格式
        x = x.flatten(2).transpose(1, 2)
        # 应用第二个KAN线性层
        x = self.fc2(x.reshape(B * N, -1)).reshape(B, N, -1)
        # 应用dropout并返回
        return self.drop(x)

class KANBlock(nn.Module):
    '''
    KAN块，包含一个LayerNorm和一个KANLayer，并使用残差连接
    '''
    def __init__(self, dim, drop=0., drop_path=0., norm_layer=nn.LayerNorm):
        super().__init__()
        # DropPath用于随机丢弃残差连接，增强正则化
        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()
        # 层归一化
        self.norm = norm_layer(dim)
        # KAN层
        self.layer = KANLayer(in_features=dim, hidden_features=dim, drop=drop)

    def forward(self, x, H, W):
        '''
        前向传播函数，实现残差连接
        '''
        return x + self.drop_path(self.layer(self.norm(x), H, W))

class PatchEmbed(nn.Module):
    '''
    图像块嵌入层，将图像转换为序列表示
    '''
    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        # 计算输出特征图的高和宽
        self.H, self.W = img_size[0] // stride, img_size[1] // stride
        # 计算图像块的数量
        self.num_patches = self.H * self.W
        # 卷积层，用于提取图像块特征
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,
                              padding=(patch_size[0] // 2, patch_size[1] // 2))
        # 层归一化
        self.norm = nn.LayerNorm(embed_dim)

    def forward(self, x):
        '''
        前向传播函数
        将图像转换为序列表示
        '''
        # 应用卷积
        x = self.proj(x)
        # 获取输出特征图的尺寸
        _, _, H, W = x.shape
        # 将特征图展平为序列
        x = x.flatten(2).transpose(1, 2)
        # 应用层归一化
        x = self.norm(x)
        return x, H, W

class VGGBlock(nn.Module):
    '''
    VGG块，包含两个卷积层，每个卷积层后跟批归一化和ReLU激活
    '''
    def __init__(self, in_channels, middle_channels, out_channels):
        super().__init__()
        # ReLU激活函数
        self.relu = nn.ReLU(inplace=True)
        # 第一个卷积层
        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)
        # 第一个批归一化层
        self.bn1 = nn.BatchNorm2d(middle_channels)
        # 第二个卷积层
        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)
        # 第二个批归一化层
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        '''
        前向传播函数
        '''
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        return out

class UKAN_NestedUNet(nn.Module):
    '''
    UKAN嵌套UNet模型，将KAN模块集成到UNet++架构中
    KAN模块主要在深层特征(x2_0, x2_2, x3_0, x4_0, x3_1)中发挥作用
    '''
    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, **kwargs):
        super().__init__()
        # 定义每层的滤波器数量
        self.nb_filter = [32, 64, 128, 256, 512]
        # 是否使用深度监督
        self.deep_supervision = deep_supervision
        # 最大池化层，用于下采样
        self.pool = nn.MaxPool2d(2, 2)
        # 上采样层，用于上采样
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        # 编码器部分
        # 第一层编码器
        self.conv0_0 = VGGBlock(input_channels, self.nb_filter[0], self.nb_filter[0])
        # 第二层编码器
        self.conv1_0 = VGGBlock(self.nb_filter[0], self.nb_filter[1], self.nb_filter[1])
        # 第三层编码器
        self.conv2_0 = VGGBlock(self.nb_filter[1], self.nb_filter[2], self.nb_filter[2])
        
        # 新增：为x2_0添加PatchEmbed和KANBlock
        self.patch_embed2 = PatchEmbed(img_size=img_size//4, patch_size=3, stride=1, in_chans=self.nb_filter[2], embed_dim=self.nb_filter[2])
        self.block2 = KANBlock(dim=self.nb_filter[2], norm_layer=nn.LayerNorm)
        
        # 第四层编码器
        self.conv3_0 = VGGBlock(self.nb_filter[2], self.nb_filter[3], self.nb_filter[3])
        # 第四层的图像块嵌入，将特征转换为序列表示以应用KAN
        self.patch_embed3 = PatchEmbed(img_size=img_size//8, patch_size=3, stride=2, in_chans=self.nb_filter[3], embed_dim=self.nb_filter[3])
        # 第四层的KAN块，增强特征表达能力
        self.block3 = KANBlock(dim=self.nb_filter[3], norm_layer=nn.LayerNorm)
        # 第五层的图像块嵌入
        self.patch_embed4 = PatchEmbed(img_size=img_size//16, patch_size=3, stride=2, in_chans=self.nb_filter[3], embed_dim=self.nb_filter[4])
        # 第五层的KAN块
        self.block4 = KANBlock(dim=self.nb_filter[4], norm_layer=nn.LayerNorm)

        # 解码器部分
        # 第一层解码器
        self.conv0_1 = VGGBlock(self.nb_filter[0]+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])
        # 第二层解码器
        self.conv1_1 = VGGBlock(self.nb_filter[1]+self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])
        # 第三层解码器
        self.conv2_1 = VGGBlock(self.nb_filter[2]+self.nb_filter[3], self.nb_filter[2], self.nb_filter[2])
        # 第四层解码器
        self.conv3_1 = VGGBlock(self.nb_filter[3]+self.nb_filter[4], self.nb_filter[3], self.nb_filter[3])
        # 第四层解码器的KAN块
        self.dblock3 = KANBlock(dim=self.nb_filter[3], norm_layer=nn.LayerNorm)
        # 嵌套连接的解码器
        self.conv0_2 = VGGBlock(self.nb_filter[0]*2+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])
        self.conv1_2 = VGGBlock(self.nb_filter[1]*2+self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])
        self.conv2_2 = VGGBlock(self.nb_filter[2]*2+self.nb_filter[3], self.nb_filter[2], self.nb_filter[2])
        # 第三层解码器的KAN块
        self.dblock2 = KANBlock(dim=self.nb_filter[2], norm_layer=nn.LayerNorm)
        # 更深层的嵌套连接
        self.conv0_3 = VGGBlock(self.nb_filter[0]*3+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])
        self.conv1_3 = VGGBlock(self.nb_filter[1]*3+self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])
        self.conv0_4 = VGGBlock(self.nb_filter[0]*4+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])

        # 为x0_1, x0_2, x0_3, x0_4特征图创建CBAM模块
        self.cbam_x0_1 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)
        self.cbam_x0_2 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)
        self.cbam_x0_3 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)
        self.cbam_x0_4 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)

        # 输出层
        if self.deep_supervision:
            # 如果使用深度监督，为每个解码器输出创建一个卷积层
            self.final1 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)
            self.final2 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)
            self.final3 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)
            self.final4 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)
        else:
            # 否则只为最终输出创建一个卷积层
            self.final = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)

    def forward(self, input):
        '''
        前向传播函数
        实现UNet++的前向传播，并在深层特征中应用KAN模块
        '''
        # 编码器路径
        # 第一层特征
        x0_0 = self.conv0_0(input)
        # 第二层特征
        x1_0 = self.conv1_0(self.pool(x0_0))
        # 第一层解码器特征
        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))
        # 应用CBAM到x0_1
        x0_1 = self.cbam_x0_1(x0_1)

        # 第三层特征（经过KAN处理）
        x2_0_raw = self.conv2_0(self.pool(x1_0))
        # 新增：将x2_0转换为序列表示，应用KAN块
        out, H, W = self.patch_embed2(x2_0_raw)
        out = self.block2(out, H, W)
        # 将序列表示转换回空间表示
        x2_0 = out.reshape(-1, H, W, self.nb_filter[2]).permute(0, 3, 1, 2).contiguous()
        
        # 第二层解码器特征
        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))
        # 第一层深度解码器特征
        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))
        # 应用CBAM到x0_2
        x0_2 = self.cbam_x0_2(x0_2)

        # 第四层特征
        x3_0 = self.conv3_0(self.pool(x2_0))
        # 将x3_0转换为序列表示，应用KAN块
        out, H, W = self.patch_embed3(x3_0)
        out = self.block3(out, H, W)
        # 将序列表示转换回空间表示
        x3_0 = out.reshape(-1, H, W, self.nb_filter[3]).permute(0, 3, 1, 2).contiguous()

        # 第五层特征
        # 将x3_0转换为序列表示，应用KAN块
        out, H, W = self.patch_embed4(x3_0)
        out = self.block4(out, H, W)
        # 将序列表示转换回空间表示
        x4_0 = out.reshape(-1, H, W, self.nb_filter[4]).permute(0, 3, 1, 2).contiguous()

        # 第四层解码器特征
        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))
        # 将x3_1转换为序列表示，应用KAN块
        _, _, H, W = x3_1.shape
        out = x3_1.flatten(2).transpose(1, 2)
        out = self.dblock3(out, H, W)
        # 将序列表示转换回空间表示
        x3_1 = out.reshape(-1, H, W, self.nb_filter[3]).permute(0, 3, 1, 2).contiguous()

        # 修复尺寸不匹配问题：对x3_1进行两次上采样
        # 从14x14上采样到28x28，再到56x56
        x3_1_up = self.up(self.up(x3_1))
        # 第三层解码器特征
        x2_1 = self.conv2_1(torch.cat([x2_0, x3_1_up], 1))
        
        # 第二层深度解码器特征
        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))
        # 第一层更深解码器特征
        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))
        # 应用CBAM到x0_3
        x0_3 = self.cbam_x0_3(x0_3)

        # 第三层深度解码器特征
        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, x3_1_up], 1))  # 使用相同的上采样结果
        # 将x2_2转换为序列表示，应用KAN块
        _, _, H, W = x2_2.shape
        out = x2_2.flatten(2).transpose(1, 2)
        out = self.dblock2(out, H, W)
        # 将序列表示转换回空间表示
        x2_2 = out.reshape(-1, H, W, self.nb_filter[2]).permute(0, 3, 1, 2).contiguous()

        # 第二层更深解码器特征
        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))
        # 最终解码器特征
        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))
        # 应用CBAM到x0_4
        x0_4 = self.cbam_x0_4(x0_4)

        # 输出处理
        if self.deep_supervision:
            # 如果使用深度监督，返回所有解码器的输出
            output1 = self.final1(x0_1)
            output2 = self.final2(x0_2)
            output3 = self.final3(x0_3)
            output4 = self.final4(x0_4)
            return [output1, output2, output3, output4]
        else:
            # 否则只返回最终输出
            output = self.final(x0_4)
            return output 
```


kan.py
```
import torch
import torch.nn.functional as F
import math


class KANLinear(torch.nn.Module):
    def __init__(
        self,
        in_features,
        out_features,
        grid_size=5,
        spline_order=3,
        scale_noise=0.1,
        scale_base=1.0,
        scale_spline=1.0,
        enable_standalone_scale_spline=True,
        base_activation=torch.nn.SiLU,
        grid_eps=0.02,
        grid_range=[-1, 1],
    ):
        super(KANLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        self.spline_order = spline_order

        h = (grid_range[1] - grid_range[0]) / grid_size
        grid = (
            (
                torch.arange(-spline_order, grid_size + spline_order + 1) * h
                + grid_range[0]
            )
            .expand(in_features, -1)
            .contiguous()
        )
        self.register_buffer('grid', grid)

        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))
        self.spline_weight = torch.nn.Parameter(
            torch.Tensor(out_features, in_features, grid_size + spline_order)
        )
        if enable_standalone_scale_spline:
            self.spline_scaler = torch.nn.Parameter(
                torch.Tensor(out_features, in_features)
            )

        self.scale_noise = scale_noise
        self.scale_base = scale_base
        self.scale_spline = scale_spline
        self.enable_standalone_scale_spline = enable_standalone_scale_spline
        self.base_activation = base_activation()
        self.grid_eps = grid_eps

        self.reset_parameters()

    def reset_parameters(self):
        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)
        with torch.no_grad():
            noise = (
                (
                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)
                    - 1 / 2
                )
                * self.scale_noise
                / self.grid_size
            )
            self.spline_weight.data.copy_(
                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)
                * self.curve2coeff(
                    self.grid.T[self.spline_order : -self.spline_order],
                    noise,
                )
            )
            if self.enable_standalone_scale_spline:
                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)
                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)

    def b_splines(self, x: torch.Tensor):
        '''
        Compute the B-spline bases for the given input tensor.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).

        Returns:
            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).
        '''
        assert x.dim() == 2 and x.size(1) == self.in_features

        grid: torch.Tensor = (
            self.grid
        )  # (in_features, grid_size + 2 * spline_order + 1)
        x = x.unsqueeze(-1)
        bases = ((x &gt;= grid[:, :-1]) &amp; (x &lt; grid[:, 1:])).to(x.dtype)
        for k in range(1, self.spline_order + 1):
            bases = (
                (x - grid[:, : -(k + 1)])
                / (grid[:, k:-1] - grid[:, : -(k + 1)])
                * bases[:, :, :-1]
            ) + (
                (grid[:, k + 1 :] - x)
                / (grid[:, k + 1 :] - grid[:, 1:(-k)])
                * bases[:, :, 1:]
            )

        assert bases.size() == (
            x.size(0),
            self.in_features,
            self.grid_size + self.spline_order,
        )
        return bases.contiguous()

    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):
        '''
        Compute the coefficients of the curve that interpolates the given points.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_features).
            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).

        Returns:
            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).
        '''
        assert x.dim() == 2 and x.size(1) == self.in_features
        assert y.size() == (x.size(0), self.in_features, self.out_features)

        A = self.b_splines(x).transpose(
            0, 1
        )  # (in_features, batch_size, grid_size + spline_order)
        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)
        solution = torch.linalg.lstsq(
            A, B
        ).solution  # (in_features, grid_size + spline_order, out_features)
        result = solution.permute(
            2, 0, 1
        )  # (out_features, in_features, grid_size + spline_order)

        assert result.size() == (
            self.out_features,
            self.in_features,
            self.grid_size + self.spline_order,
        )
        return result.contiguous()

    @property
    def scaled_spline_weight(self):
        return self.spline_weight * (
            self.spline_scaler.unsqueeze(-1)
            if self.enable_standalone_scale_spline
            else 1.0
        )

    def forward(self, x: torch.Tensor):
        assert x.dim() == 2 and x.size(1) == self.in_features

        base_output = F.linear(self.base_activation(x), self.base_weight)
        spline_output = F.linear(
            self.b_splines(x).view(x.size(0), -1),
            self.scaled_spline_weight.view(self.out_features, -1),
        )
        return base_output + spline_output

    @torch.no_grad()
    def update_grid(self, x: torch.Tensor, margin=0.01):
        assert x.dim() == 2 and x.size(1) == self.in_features
        batch = x.size(0)

        splines = self.b_splines(x)  # (batch, in, coeff)
        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)
        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)
        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)
        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)
        unreduced_spline_output = unreduced_spline_output.permute(
            1, 0, 2
        )  # (batch, in, out)

        # sort each channel individually to collect data distribution
        x_sorted = torch.sort(x, dim=0)[0]
        grid_adaptive = x_sorted[
            torch.linspace(
                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device
            )
        ]

        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size
        grid_uniform = (
            torch.arange(
                self.grid_size + 1, dtype=torch.float32, device=x.device
            ).unsqueeze(1)
            * uniform_step
            + x_sorted[0]
            - margin
        )

        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive
        grid = torch.concatenate(
            [
                grid[:1]
                - uniform_step
                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),
                grid,
                grid[-1:]
                + uniform_step
                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),
            ],
            dim=0,
        )

        self.grid.copy_(grid.T)
        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))

    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):
        '''
        Compute the regularization loss.

        This is a dumb simulation of the original L1 regularization as stated in the
        paper, since the original one requires computing absolutes and entropy from the
        expanded (batch, in_features, out_features) intermediate tensor, which is hidden
        behind the F.linear function if we want an memory efficient implementation.

        The L1 regularization is now computed as mean absolute value of the spline
        weights. The authors implementation also includes this term in addition to the
        sample-based regularization.
        '''
        l1_fake = self.spline_weight.abs().mean(-1)
        regularization_loss_activation = l1_fake.sum()
        p = l1_fake / regularization_loss_activation
        regularization_loss_entropy = -torch.sum(p * p.log())
        return (
            regularize_activation * regularization_loss_activation
            + regularize_entropy * regularization_loss_entropy
        )


class KAN(torch.nn.Module):
    def __init__(
        self,
        layers_hidden,
        grid_size=5,
        spline_order=3,
        scale_noise=0.1,
        scale_base=1.0,
        scale_spline=1.0,
        base_activation=torch.nn.SiLU,
        grid_eps=0.02,
        grid_range=[-1, 1],
    ):
        super(KAN, self).__init__()
        self.grid_size = grid_size
        self.spline_order = spline_order

        self.layers = torch.nn.ModuleList()
        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):
            self.layers.append(
                KANLinear(
                    in_features,
                    out_features,
                    grid_size=grid_size,
                    spline_order=spline_order,
                    scale_noise=scale_noise,
                    scale_base=scale_base,
                    scale_spline=scale_spline,
                    base_activation=base_activation,
                    grid_eps=grid_eps,
                    grid_range=grid_range,
                )
            )

    def forward(self, x: torch.Tensor, update_grid=False):
        for layer in self.layers:
            if update_grid:
                layer.update_grid(x)
            x = layer(x)
        return x

    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):
        return sum(
            layer.regularization_loss(regularize_activation, regularize_entropy)
            for layer in self.layers
        )
```


dataset.py
```
import os

import cv2
import numpy as np
import torch
import torch.utils.data


class Dataset(torch.utils.data.Dataset):
    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, num_classes, transform=None):
        '''
        Args:
            img_ids (list): Image ids.
            img_dir: Image file directory.
            mask_dir: Mask file directory.
            img_ext (str): Image file extension.
            mask_ext (str): Mask file extension.
            num_classes (int): Number of classes.
            transform (Compose, optional): Compose transforms of albumentations. Defaults to None.
        
        Note:
            Make sure to put the files as the following structure:
            &lt;dataset name&gt;
            ├── images
            |   ├── 0a7e06.jpg
            │   ├── 0aab0a.jpg
            │   ├── 0b1761.jpg
            │   ├── ...
            |
            └── masks
                ├── 0
                |   ├── 0a7e06.png
                |   ├── 0aab0a.png
                |   ├── 0b1761.png
                |   ├── ...
                |
                ├── 1
                |   ├── 0a7e06.png
                |   ├── 0aab0a.png
                |   ├── 0b1761.png
                |   ├── ...
                ...
        '''
        self.img_ids = img_ids
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.img_ext = img_ext
        self.mask_ext = mask_ext
        self.num_classes = num_classes
        self.transform = transform

    def __len__(self):
        return len(self.img_ids)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        
        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))

        mask = []
        for i in range(self.num_classes):
            mask.append(cv2.imread(os.path.join(self.mask_dir, str(i),
                        img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])
        #数组沿深度方向进行拼接。</description><guid isPermaLink="true">https://Aloner63.github.io/post/kc-unet%2B%2B-yuan-ma.html</guid><pubDate>Thu, 18 Dec 2025 01:28:50 +0000</pubDate></item><item><title>vscode实现远程开发</title><link>https://Aloner63.github.io/post/vscode-shi-xian-yuan-cheng-kai-fa.html</link><description>##### 原理：

VSCode 的 Remote - SSH 功能本质上是利用 SSH 协议，在本地机器（客户端）和远程云服务器之间建立安全连接，然后在远程服务器上运行一个 VSCode Server 实例。</description><guid isPermaLink="true">https://Aloner63.github.io/post/vscode-shi-xian-yuan-cheng-kai-fa.html</guid><pubDate>Tue, 11 Mar 2025 10:33:52 +0000</pubDate></item><item><title>FreeRTOS_2</title><link>https://Aloner63.github.io/post/FreeRTOS_2.html</link><description>
[TOC]

## 11.FreeRTOS任务相关的其他API函数

### 一、FreeRTOS任务相关的其他API函数介绍

#### 1、FreeRTOS任务相关API函数介绍(部分常用的)

答：

![](https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/freertos/%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E7%9A%84%E5%85%B6%E4%BB%96API%E5%87%BD%E6%95%B0.png)

### 二、任务状态查询API函数

#### 1、获取任务优先级函数

答：

```C
UBaseType_t  uxTaskPriorityGet(  const TaskHandle_t xTask  )
```

此函数用于获取指定任务的任务优先级，使用该函数需要将宏 INCLUDE_uxTaskPriorityGet 置1。</description><guid isPermaLink="true">https://Aloner63.github.io/post/FreeRTOS_2.html</guid><pubDate>Wed, 05 Mar 2025 06:04:39 +0000</pubDate></item><item><title>FreeRTOS_1</title><link>https://Aloner63.github.io/post/FreeRTOS_1.html</link><description>
## 目录

&lt;details&gt;
&lt;summary&gt;概述&lt;/summary&gt;

- [概述](#overview)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;1.基础知识&lt;/summary&gt;

- [1.基础知识](#basic-knowledge)
  - [一.任务调度器简述](#task-scheduler-overview)
    - [1.什么是任务调度器](#what-is-task-scheduler)
    - [2.freertos的调度方式](#freertos-scheduling-methods)
    - [3.抢占式调度过程](#preemptive-scheduling-process)
    - [4.时间片是什么](#what-is-time-slice)
    - [5.时间片调度过程](#time-slice-scheduling-process)
  - [二.任务状态](#task-states)
    - [1.freertos的任务状态](#freertos-task-states)
    - [2.四种状态之间的转换关系](#task-state-transitions)
    - [3.任务状态列表](#task-state-list)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2.freertos系统配置文件详解&lt;/summary&gt;

- [2.freertos系统配置文件详解](#freertos-config-details)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;3.任务的创建和删除&lt;/summary&gt;

- [3.任务的创建和删除](#task-creation-and-deletion)
  - [一.任务创建和删除API函数](#task-creation-deletion-api)
    - [1.任务创建和删除的本质](#task-creation-deletion-essence)
    - [2.任务动态创建和静态创建的区别](#dynamic-vs-static-task-creation)
    - [3.任务控制块结构体成员介绍](#task-control-block-members)
    - [4.什么是临界保护区](#what-is-critical-section)
    - [5.动态创建的优点](#dynamic-creation-advantages)
    - [6.静态创建的优点](#static-creation-advantages)
  - [二.任务的创建（动态）](#dynamic-task-creation)
    - [1.动态函数的创建](#dynamic-function-creation)
    - [2.什么是句柄](#what-is-handle)
    - [3.实现动态创建任务流程](#dynamic-task-creation-process)
    - [4.动态任务创建函数内部实现简述](#dynamic-task-creation-internal)
  - [三.任务的创建（静态）](#static-task-creation)
    - [1.静态函数的创建](#static-function-creation)
    - [2.实现静态创建任务流程](#static-task-creation-process)
    - [3.静态任务创建函数内部实现简述](#static-task-creation-internal)
  - [四.任务的删除](#task-deletion)
    - [1.任务删除函数](#task-deletion-function)
    - [2.删除任务流程](#task-deletion-process)
    - [3.删除任务函数内部实现简述](#task-deletion-internal)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;4.任务的挂起和恢复&lt;/summary&gt;

- [4.任务的挂起和恢复](#task-suspension-and-resumption)
  - [一.任务的挂起和恢复介绍](#task-suspension-resumption-intro)
  - [二.任务的挂起](#task-suspension)
    - [1.挂起函数介绍](#suspension-function-intro)
    - [2.任务挂起函数内部实现](#suspension-function-internal)
  - [三.任务的恢复](#task-resumption)
    - [1.任务恢复函数介绍（任务中）](#resumption-function-task-intro)
    - [2.任务回复函数的实现（任务中）](#resumption-function-task-internal)
    - [3.任务恢复函数介绍（中断中）](#resumption-function-isr-intro)
    - [4.任务恢复函数内部实现（中断中）](#resumption-function-isr-internal)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;5.中断管理&lt;/summary&gt;

- [5.中断管理](#interrupt-management)
  - [一.中断介绍](#interrupt-intro)
    - [1.什么是中断](#what-is-interrupt)
    - [2.中断执行机制](#interrupt-execution-mechanism)
  - [二.中断优先级分组设置](#interrupt-priority-grouping)
    - [1.中断优先级分组介绍](#priority-grouping-intro)
    - [2.什么是去抢占优先级什么是子优先级](#preemption-vs-subpriority)
    - [3.中断优先级配置方式](#priority-configuration-methods)
    - [4.freertos中对中断优先级的管理](#freertos-interrupt-priority-management)
  - [三.中断相关寄存器](#interrupt-related-registers)
    - [1.系统中断优先级配置寄存器](#system-interrupt-priority-registers)
    - [2.FreeRTOS如何配置PendSV和Systick中断优先级](#freertos-pendsv-systick-config)
    - [3.为什么将PendSV和SysTick设置最低优先级](#why-lowest-pendsv-systick)
    - [4.中断屏蔽寄存器](#interrupt-mask-registers)
    - [5.BASEPRI中断屏蔽寄存器](#basepri-interrupt-mask)
    - [6.freertos的关闭中断程序](#freertos-disable-interrupts)
    - [7.freertos的开中断程序](#freertos-enable-interrupts)
    - [8.中断服务函数调用FreeRTOS的API函数需注意](#freertos-isr-api-notes)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;6.freertos临界段代码保护&lt;/summary&gt;

- [6.freertos临界段代码保护](#freertos-critical-section-protection)
  - [1.什么是临界段](#what-is-critical-section-1)
  - [2.适用什么场合](#critical-section-use-cases)
  - [3.什么可以打断当前程序的运行](#what-interrupts-program)
  - [4.临界段代码保护函数](#critical-section-protection-functions)
  - [5.临界段代码保护函数使用特点](#critical-section-function-features)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;7.任务调度器挂起和恢复函数&lt;/summary&gt;

- [7.任务调度器挂起和恢复函数](#scheduler-suspend-resume-functions)
  - [1.任务调度器挂起和恢复函数](#scheduler-suspend-resume-functions-1)
  - [2.任务调度器挂起和恢复的特点](#scheduler-suspend-resume-features)
  - [3.挂起任务调度器vTaskSuspendAll](#suspend-scheduler-vtasksuspendall)
  - [4.恢复任务调度器xTaskResumeAll](#resume-scheduler-xtaskresumeall)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;8.freertos的列表和列表项&lt;/summary&gt;

- [8.freertos的列表和列表项](#freertos-lists-and-items)
  - [一.列表和列表项的简介](#list-and-item-intro)
    - [1.什么是列表](#what-is-list)
    - [2.什么是列表项](#what-is-list-item)
    - [3.列表和列表项的关系](#list-and-item-relationship)
    - [4.列表链表和数组的区别](#list-vs-array)
    - [5.OS中为什么使用列表](#why-use-lists-in-os)
    - [6.列表结构体介绍](#list-structure-intro)
    - [7.列表项结构体介绍](#list-item-structure-intro)
    - [8.迷你列表项](#mini-list-item)
    - [9.列表和列表项关系事例](#list-and-item-example)
  - [二.列表相关的API函数介绍](#list-related-api-intro)
    - [1.列表API函数](#list-api-functions)
    - [2.初始化列表函数vListInitialise](#init-list-vlistinitialise)
    - [3.初始化列表项函数vListInitialiseItem](#init-list-item-vlistinitialiseitem)
    - [4.列表插入列表项函数vListInsert](#insert-list-vlistinsert)
    - [5.列表末尾插入列表项vListInsertEnd](#insert-end-vlistinsertend)
    - [6.列表项移除函数uxListRemove](#remove-list-uxlistremove)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;9.freertos任务调度&lt;/summary&gt;

- [9.freertos任务调度](#freertos-task-scheduling)
  - [一.开启任务调度器熟悉](#start-scheduler-overview)
    - [1.开启任务调度器函数vTaskStartScheduler](#start-scheduler-vtaskstartscheduler)
    - [2.配置硬件架构及启动第一个任务函数xPortStartScheduler](#config-hardware-xportstartscheduler)
    - [3.SysTick滴答定时器](#systick-timer)
    - [4.堆和栈的地址生长方向](#heap-stack-growth)
    - [5.压栈和出栈的地址增长方向](#stack-push-pop-direction)
    - [6.知识补充](#knowledge-supplement)
  - [二.启动第一个任务熟悉](#start-first-task-overview)
    - [1.启动第一个任务涉及的关键函数](#start-first-task-key-functions)
    - [2.想象一下应该如何启动第一个任务](#how-to-start-first-task)
    - [3.prvStartFirstTask 介绍](#prvstartfirsttask-intro)
    - [4.什么是MSP指针](#what-is-msp-pointer)
    - [5.为什么汇编代码要PRESERVE8八字节对齐](#why-preserve8-alignment)
    - [6.prvStartFirstTask为什么要操作0XE00ED08](#prvstartfirsttask-0xe00ed08)
    - [7.vPortSVCHandle介绍](#vportsvchandle-intro)
    - [8.出栈压栈汇编指令详解](#stack-instruction-details)
  - [三.任务切换掌握](#task-switching-mastery)
    - [1.任务切换的本质](#task-switching-essence)
    - [2.任务切换过程](#task-switching-process)
    - [3.PendSV中断是如何触发的](#pendsv-trigger)
    - [4.在PendSV中断中PSP和MSP](#pendsv-psp-msp)
    - [5.查找最高优先级任务](#find-highest-priority-task)
    - [6.前导置零指令](#leading-zero-instruction)
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;10.FreeRTOS时间片轮询&lt;/summary&gt;

- [10.FreeRTOS时间片轮询](#freertos-timeslice-polling)
  - [一.时间片轮询简介](#timeslice-polling-intro)
&lt;/details&gt;

---

## &lt;a id='freertos'&gt;&lt;/a&gt;FreeRTOS

## &lt;a id='overview'&gt;&lt;/a&gt;概述

随着产品实现的功能越来越多，单纯的裸机系统已经不能完美的解决问题了，反而会使程序边的更加复杂，如果想降低编程的难度，我们可以考虑引入RTOS实现多任务管理。</description><guid isPermaLink="true">https://Aloner63.github.io/post/FreeRTOS_1.html</guid><pubDate>Sun, 02 Mar 2025 09:16:59 +0000</pubDate></item><item><title>vscode运行c++</title><link>https://Aloner63.github.io/post/vscode-yun-xing-c%2B%2B.html</link><description>#### vscode运行c++

###### 在c++的编译中，g++，gdb，gcc都是什么？

**GCC (GNU Compiler Collection)**，这是GNU项目的编译器集合，最初是'GNU C Compiler'的缩写，现在支持多种编程语言，**主要用于编译C语言程序**，命令格式：gcc source.c -o output

**G++ (GNU C++ Compiler)**是GCC的一部分，**专门用于编译C++程序**，会自动链接C++标准库，将.cpp文件视为C++源代码（而gcc默认将其视为C文件），命令格式：g++ source.cpp -o output

**GDB (GNU Debugger)**是GNU项目的调试器，用于程序调试，可以：（设置断点，单步执行，查看变量值，查看调用栈，监控变量变化）命令格式：gdb ./program

GNU项目（GNU Project）是一个非常重要的自由软件运动。</description><guid isPermaLink="true">https://Aloner63.github.io/post/vscode-yun-xing-c%2B%2B.html</guid><pubDate>Tue, 11 Feb 2025 10:42:18 +0000</pubDate></item><item><title>esp8266连接私有服务器</title><link>https://Aloner63.github.io/post/esp8266-lian-jie-si-you-fu-wu-qi.html</link><description>ESP-01S 基本参数&#13;
![img](https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/222a8a308874efa976dbd68125140a57.png)&#13;
&#13;
![img](https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/ab7b3ba25b06f9e259135eb4b087300d.png)&#13;
&#13;
环境：win11 ，Arduous版本2.3.4   ， ESP01S ，USB to TTL&#13;
&#13;
1，安装好Arduous IDE（这个没什么好说的，修改一下安装路径。</description><guid isPermaLink="true">https://Aloner63.github.io/post/esp8266-lian-jie-si-you-fu-wu-qi.html</guid><pubDate>Thu, 02 Jan 2025 14:21:57 +0000</pubDate></item><item><title>I2C</title><link>https://Aloner63.github.io/post/I2C.html</link><description>I2C 是一种串行通信协议。</description><guid isPermaLink="true">https://Aloner63.github.io/post/I2C.html</guid><pubDate>Thu, 02 Jan 2025 13:09:09 +0000</pubDate></item><item><title>打包python</title><link>https://Aloner63.github.io/post/da-bao-python.html</link><description>##### 将python打包成可执行exe

将python打包的方式大概分为2种，其中每一种都可以简单打包和压缩打包（压缩打包就是构件单独的环境，将项目不需要的包隔离出去）

1. 单个文件的打包
2. 多个文件的打包（当面对一个大项目的时候，为了方便维护，通常将代码分到不同的文件中。</description><guid isPermaLink="true">https://Aloner63.github.io/post/da-bao-python.html</guid><pubDate>Thu, 02 Jan 2025 07:59:57 +0000</pubDate></item><item><title>websocket</title><link>https://Aloner63.github.io/post/websocket.html</link><description>&lt;html&gt;&lt;body&gt;&#13;
&lt;!--StartFragment--&gt;&lt;p&gt;WebSocket 是一种网络通信协议，旨在通过持久的、全双工（双向）通信连接实现实时数据交换。</description><guid isPermaLink="true">https://Aloner63.github.io/post/websocket.html</guid><pubDate>Mon, 11 Nov 2024 13:15:01 +0000</pubDate></item><item><title>关于github克隆仓库出错问题</title><link>https://Aloner63.github.io/post/guan-yu-github-ke-long-cang-ku-chu-cuo-wen-ti.html</link><description>使用 git clone 下载 Github 等网站的仓库时，可能会遇到类似 'Recv failure: Connection was reset' 或 'Failed to connect to http://github.com port 443 after 21114 ms: Couldn't connect to server' 的报错。</description><guid isPermaLink="true">https://Aloner63.github.io/post/guan-yu-github-ke-long-cang-ku-chu-cuo-wen-ti.html</guid><pubDate>Sun, 27 Oct 2024 12:33:21 +0000</pubDate></item><item><title>CDN和反向代理浅解</title><link>https://Aloner63.github.io/post/CDN-he-fan-xiang-dai-li-qian-jie.html</link><description>#### CDN&#13;
&#13;
CDN的全称为“Content Delivery Network”，及，内容分发网络。</description><guid isPermaLink="true">https://Aloner63.github.io/post/CDN-he-fan-xiang-dai-li-qian-jie.html</guid><pubDate>Wed, 23 Oct 2024 12:54:57 +0000</pubDate></item><item><title>apache反向代理（并启用https）</title><link>https://Aloner63.github.io/post/apache-fan-xiang-dai-li-%EF%BC%88-bing-qi-yong-https%EF%BC%89.html</link><description>&#13;
&#13;
```&#13;
2024/11/15 更新&#13;
&#13;
可以使用cloudfare启用https(较为简单，更新后的DNS可能需要1天到两天更新。</description><guid isPermaLink="true">https://Aloner63.github.io/post/apache-fan-xiang-dai-li-%EF%BC%88-bing-qi-yong-https%EF%BC%89.html</guid><pubDate>Tue, 22 Oct 2024 03:26:05 +0000</pubDate></item><item><title>ubuntu笔记</title><link>https://Aloner63.github.io/post/ubuntu-bi-ji.html</link><description>ubuntu笔记&#13;
&#13;
**一切皆文件**&#13;
&#13;
不同颜色代表不同类型的文件&#13;
&#13;
- `蓝色`：目录&#13;
- `绿色`：可执行文件&#13;
- `白色`：一般性文件，如文本文件，配置文件等&#13;
- `红色`：压缩文件或归档文件&#13;
- `浅蓝色`：链接文件&#13;
- 红色闪烁：链接文件存在问题&#13;
- 黄色：设备文件&#13;
- 青黄色：管道文件&#13;
&#13;
##### 终端命令格式&#13;
&#13;
```&#13;
command 	[-options]	[parameter]&#13;
```&#13;
&#13;
##### 查阅命令的使用手册&#13;
&#13;
```&#13;
command	--help&#13;
```&#13;
&#13;
##### 自动补全&#13;
&#13;
在敲出 文件/目录/命令 的前几个字母后，按下tab键&#13;
&#13;
- 如果输入没有歧义，则系统自动补全&#13;
- 如果存在相似名称，再按一下`tab`键，系统会提示可能存在的命令&#13;
&#13;
##### 曾经使用过的命令&#13;
&#13;
- 按 上/下 光标键切换&#13;
- 使用`ctrl+c`退出选择，另起一行&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
##### 6个常用的终端命令&#13;
&#13;
```&#13;
ls：（list）查看当前文件夹下的内容&#13;
&#13;
pwd：（print work directoy）查看当前所在文件夹&#13;
&#13;
cd：（change directoy）移动到摸一个指定文件夹&#13;
&#13;
touch：（touch）如果文件不存在，新建文件&#13;
&#13;
mkdir：（make dirctory）创建目录&#13;
&#13;
rm：（remove）删除指定文件名&#13;
&#13;
clear：（clear）清屏&#13;
```&#13;
&#13;
&#13;
&#13;
```&#13;
ctrl+shift+=：放大终端窗口的字体&#13;
&#13;
ctrl+-：缩小&#13;
```&#13;
&#13;
&#13;
&#13;
##### ls命令&#13;
&#13;
```&#13;
ls       # 仅列出当前目录可见文件&#13;
ls -l    # 列出当前目录可见文件详细信息&#13;
ls -hl   # 列出详细信息并以可读大小显示文件大小&#13;
ls -al   # 列出所有文件（包括隐藏）的详细信息&#13;
ls --human-readable --size -1 -S --classify # 按文件大小排序&#13;
du -sh * | sort -h # 按文件大小排序(同上)&#13;
```&#13;
&#13;
&#13;
&#13;
##### cd命令&#13;
&#13;
```&#13;
cd    # 进入用户主目录；&#13;
cd /  # 进入根目录&#13;
cd ~  # 进入用户主目录；&#13;
cd ..  # 返回上级目录（若当前目录为“/“，则执行完后还在“/'；'..'为上级目录的意思）；&#13;
cd ../..  # 返回上两级目录；&#13;
cd !$  # 把上个命令的参数作为cd参数使用。</description><guid isPermaLink="true">https://Aloner63.github.io/post/ubuntu-bi-ji.html</guid><pubDate>Thu, 10 Oct 2024 01:20:01 +0000</pubDate></item><item><title>vsode（基于官方文档）配置c/c++环境</title><link>https://Aloner63.github.io/post/vsode%EF%BC%88-ji-yu-guan-fang-wen-dang-%EF%BC%89-pei-zhi-c-c%2B%2B-huan-jing.html</link><description>**从根本上来说，vscode就是一个文本编译器。</description><guid isPermaLink="true">https://Aloner63.github.io/post/vsode%EF%BC%88-ji-yu-guan-fang-wen-dang-%EF%BC%89-pei-zhi-c-c%2B%2B-huan-jing.html</guid><pubDate>Wed, 09 Oct 2024 11:23:08 +0000</pubDate></item><item><title>git的使用</title><link>https://Aloner63.github.io/post/git-de-shi-yong.html</link><description>### git&#13;
&#13;
所有的版本控制系统，其实只能跟踪文本文件的改动，比如TXT文件，网页，所有的程序代码等等，Git也不例外。</description><guid isPermaLink="true">https://Aloner63.github.io/post/git-de-shi-yong.html</guid><pubDate>Thu, 26 Sep 2024 03:14:01 +0000</pubDate></item><item><title>OSI 7层网络模型</title><link>https://Aloner63.github.io/post/OSI%207-ceng-wang-luo-mo-xing.html</link><description>1，OSI 7层网络模型&#13;
  OSI（Open System Interconnect）七层模型是一种将计算机网络通信协议划分为七个不同层次的标准化框架。</description><guid isPermaLink="true">https://Aloner63.github.io/post/OSI%207-ceng-wang-luo-mo-xing.html</guid><pubDate>Thu, 12 Sep 2024 05:16:24 +0000</pubDate></item><item><title>为什么要用Docker？</title><link>https://Aloner63.github.io/post/wei-shen-me-yao-yong-Docker%EF%BC%9F.html</link><description>一个软件，从诞生到正常使用，需要经过跟多步骤。</description><guid isPermaLink="true">https://Aloner63.github.io/post/wei-shen-me-yao-yong-Docker%EF%BC%9F.html</guid><pubDate>Tue, 03 Sep 2024 06:59:59 +0000</pubDate></item><item><title>自建vpn</title><link>https://Aloner63.github.io/post/zi-jian-vpn.html</link><description>第一步：获取vps服务器。</description><guid isPermaLink="true">https://Aloner63.github.io/post/zi-jian-vpn.html</guid><pubDate>Thu, 22 Aug 2024 04:32:39 +0000</pubDate></item><item><title>IP相关知识点，TCP/IP知识点</title><link>https://Aloner63.github.io/post/IP-xiang-guan-zhi-shi-dian-%EF%BC%8CTCP-IP-zhi-shi-dian.html</link><description>IP地址的分类系统（A类、B类、C类、D类和E类）用于简化互联网中的地址分配。</description><guid isPermaLink="true">https://Aloner63.github.io/post/IP-xiang-guan-zhi-shi-dian-%EF%BC%8CTCP-IP-zhi-shi-dian.html</guid><pubDate>Sat, 13 Jul 2024 07:51:30 +0000</pubDate></item><item><title>计算机网络基础知识</title><link>https://Aloner63.github.io/post/ji-suan-ji-wang-luo-ji-chu-zhi-shi.html</link><description>国际标准化组织（ISO）在1978年提出了'开放系统互联参考模型'，即著名的OSI/RM模型（Open System Interconnection/Reference Model）。</description><guid isPermaLink="true">https://Aloner63.github.io/post/ji-suan-ji-wang-luo-ji-chu-zhi-shi.html</guid><pubDate>Sat, 13 Jul 2024 05:10:18 +0000</pubDate></item></channel></rss>