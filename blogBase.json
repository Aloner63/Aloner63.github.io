{"singlePage": [], "startSite": "", "filingNum": "", "onePageListNum": 15, "commentLabelColor": "#006b75", "yearColorList": ["#bc4c00", "#0969da", "#1f883d", "#A333D0"], "i18n": "CN", "themeMode": "manual", "dayTheme": "light", "nightTheme": "dark", "urlMode": "pinyin", "script": "", "style": "", "head": "", "indexScript": "", "indexStyle": "", "bottomText": "", "showPostSource": 1, "iconList": {}, "UTC": 8, "rssSplit": "sentence", "exlink": {}, "needComment": 1, "allHead": "", "title": "Aloner63 \u7684\u4e2a\u4eba\u535a\u5ba2", "subTitle": "\u5206\u4eab\u6280\u672f\uff0c\u8bb0\u5f55\u751f\u6d3b\uff0c\u63a2\u7d22\u672a\u77e5", "avatarUrl": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "GMEEK_VERSION": "last", "postListJson": {"P2": {"htmlDir": "docs/post/ji-suan-ji-wang-luo-ji-chu-zhi-shi.html", "labels": ["documentation"], "postTitle": "\u8ba1\u7b97\u673a\u7f51\u7edc\u57fa\u7840\u77e5\u8bc6", "postUrl": "post/ji-suan-ji-wang-luo-ji-chu-zhi-shi.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/2", "commentNum": 0, "wordCount": 4614, "description": "\u56fd\u9645\u6807\u51c6\u5316\u7ec4\u7ec7\uff08ISO\uff09\u57281978\u5e74\u63d0\u51fa\u4e86'\u5f00\u653e\u7cfb\u7edf\u4e92\u8054\u53c2\u8003\u6a21\u578b'\uff0c\u5373\u8457\u540d\u7684OSI/RM\u6a21\u578b\uff08Open System Interconnection/Reference Model\uff09\u3002", "top": 0, "createdAt": 1720847418, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-07-13", "dateLabelColor": "#bc4c00"}, "P4": {"htmlDir": "docs/post/IP-xiang-guan-zhi-shi-dian-\uff0cTCP-IP-zhi-shi-dian.html", "labels": ["documentation"], "postTitle": "IP\u76f8\u5173\u77e5\u8bc6\u70b9\uff0cTCP/IP\u77e5\u8bc6\u70b9", "postUrl": "post/IP-xiang-guan-zhi-shi-dian-%EF%BC%8CTCP-IP-zhi-shi-dian.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/4", "commentNum": 0, "wordCount": 5705, "description": "IP\u5730\u5740\u7684\u5206\u7c7b\u7cfb\u7edf\uff08A\u7c7b\u3001B\u7c7b\u3001C\u7c7b\u3001D\u7c7b\u548cE\u7c7b\uff09\u7528\u4e8e\u7b80\u5316\u4e92\u8054\u7f51\u4e2d\u7684\u5730\u5740\u5206\u914d\u3002", "top": 0, "createdAt": 1720857090, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-07-13", "dateLabelColor": "#bc4c00"}, "P5": {"htmlDir": "docs/post/zi-jian-vpn.html", "labels": ["course"], "postTitle": "\u81ea\u5efavpn", "postUrl": "post/zi-jian-vpn.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/5", "commentNum": 0, "wordCount": 6323, "description": "\u7b2c\u4e00\u6b65\uff1a\u83b7\u53d6vps\u670d\u52a1\u5668\u3002", "top": 0, "createdAt": 1724301159, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-08-22", "dateLabelColor": "#bc4c00"}, "P6": {"htmlDir": "docs/post/wei-shen-me-yao-yong-Docker\uff1f.html", "labels": ["documentation"], "postTitle": "\u4e3a\u4ec0\u4e48\u8981\u7528Docker\uff1f", "postUrl": "post/wei-shen-me-yao-yong-Docker%EF%BC%9F.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/6", "commentNum": 0, "wordCount": 830, "description": "\u4e00\u4e2a\u8f6f\u4ef6\uff0c\u4ece\u8bde\u751f\u5230\u6b63\u5e38\u4f7f\u7528\uff0c\u9700\u8981\u7ecf\u8fc7\u8ddf\u591a\u6b65\u9aa4\u3002", "top": 0, "createdAt": 1725346799, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-09-03", "dateLabelColor": "#bc4c00"}, "P7": {"htmlDir": "docs/post/OSI 7-ceng-wang-luo-mo-xing.html", "labels": ["documentation"], "postTitle": "OSI 7\u5c42\u7f51\u7edc\u6a21\u578b", "postUrl": "post/OSI%207-ceng-wang-luo-mo-xing.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/7", "commentNum": 0, "wordCount": 3680, "description": "1\uff0cOSI 7\u5c42\u7f51\u7edc\u6a21\u578b\r\n  OSI\uff08Open System Interconnect\uff09\u4e03\u5c42\u6a21\u578b\u662f\u4e00\u79cd\u5c06\u8ba1\u7b97\u673a\u7f51\u7edc\u901a\u4fe1\u534f\u8bae\u5212\u5206\u4e3a\u4e03\u4e2a\u4e0d\u540c\u5c42\u6b21\u7684\u6807\u51c6\u5316\u6846\u67b6\u3002", "top": 0, "createdAt": 1726118184, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-09-12", "dateLabelColor": "#bc4c00"}, "P8": {"htmlDir": "docs/post/git-de-shi-yong.html", "labels": ["course"], "postTitle": "git\u7684\u4f7f\u7528", "postUrl": "post/git-de-shi-yong.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/8", "commentNum": 0, "wordCount": 8098, "description": "### git\r\n\r\n\u6240\u6709\u7684\u7248\u672c\u63a7\u5236\u7cfb\u7edf\uff0c\u5176\u5b9e\u53ea\u80fd\u8ddf\u8e2a\u6587\u672c\u6587\u4ef6\u7684\u6539\u52a8\uff0c\u6bd4\u5982TXT\u6587\u4ef6\uff0c\u7f51\u9875\uff0c\u6240\u6709\u7684\u7a0b\u5e8f\u4ee3\u7801\u7b49\u7b49\uff0cGit\u4e5f\u4e0d\u4f8b\u5916\u3002", "top": 0, "createdAt": 1727320441, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-09-26", "dateLabelColor": "#bc4c00"}, "P9": {"htmlDir": "docs/post/vsode\uff08-ji-yu-guan-fang-wen-dang-\uff09-pei-zhi-c-c++-huan-jing.html", "labels": ["course"], "postTitle": "vsode\uff08\u57fa\u4e8e\u5b98\u65b9\u6587\u6863\uff09\u914d\u7f6ec/c++\u73af\u5883", "postUrl": "post/vsode%EF%BC%88-ji-yu-guan-fang-wen-dang-%EF%BC%89-pei-zhi-c-c%2B%2B-huan-jing.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/9", "commentNum": 0, "wordCount": 4220, "description": "**\u4ece\u6839\u672c\u4e0a\u6765\u8bf4\uff0cvscode\u5c31\u662f\u4e00\u4e2a\u6587\u672c\u7f16\u8bd1\u5668\u3002", "top": 0, "createdAt": 1728472988, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-10-09", "dateLabelColor": "#bc4c00"}, "P10": {"htmlDir": "docs/post/ubuntu-bi-ji.html", "labels": ["course"], "postTitle": "ubuntu\u7b14\u8bb0", "postUrl": "post/ubuntu-bi-ji.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/10", "commentNum": 0, "wordCount": 7276, "description": "ubuntu\u7b14\u8bb0\r\n\r\n**\u4e00\u5207\u7686\u6587\u4ef6**\r\n\r\n\u4e0d\u540c\u989c\u8272\u4ee3\u8868\u4e0d\u540c\u7c7b\u578b\u7684\u6587\u4ef6\r\n\r\n- `\u84dd\u8272`\uff1a\u76ee\u5f55\r\n- `\u7eff\u8272`\uff1a\u53ef\u6267\u884c\u6587\u4ef6\r\n- `\u767d\u8272`\uff1a\u4e00\u822c\u6027\u6587\u4ef6\uff0c\u5982\u6587\u672c\u6587\u4ef6\uff0c\u914d\u7f6e\u6587\u4ef6\u7b49\r\n- `\u7ea2\u8272`\uff1a\u538b\u7f29\u6587\u4ef6\u6216\u5f52\u6863\u6587\u4ef6\r\n- `\u6d45\u84dd\u8272`\uff1a\u94fe\u63a5\u6587\u4ef6\r\n- \u7ea2\u8272\u95ea\u70c1\uff1a\u94fe\u63a5\u6587\u4ef6\u5b58\u5728\u95ee\u9898\r\n- \u9ec4\u8272\uff1a\u8bbe\u5907\u6587\u4ef6\r\n- \u9752\u9ec4\u8272\uff1a\u7ba1\u9053\u6587\u4ef6\r\n\r\n##### \u7ec8\u7aef\u547d\u4ee4\u683c\u5f0f\r\n\r\n```\r\ncommand \t[-options]\t[parameter]\r\n```\r\n\r\n##### \u67e5\u9605\u547d\u4ee4\u7684\u4f7f\u7528\u624b\u518c\r\n\r\n```\r\ncommand\t--help\r\n```\r\n\r\n##### \u81ea\u52a8\u8865\u5168\r\n\r\n\u5728\u6572\u51fa \u6587\u4ef6/\u76ee\u5f55/\u547d\u4ee4 \u7684\u524d\u51e0\u4e2a\u5b57\u6bcd\u540e\uff0c\u6309\u4e0btab\u952e\r\n\r\n- \u5982\u679c\u8f93\u5165\u6ca1\u6709\u6b67\u4e49\uff0c\u5219\u7cfb\u7edf\u81ea\u52a8\u8865\u5168\r\n- \u5982\u679c\u5b58\u5728\u76f8\u4f3c\u540d\u79f0\uff0c\u518d\u6309\u4e00\u4e0b`tab`\u952e\uff0c\u7cfb\u7edf\u4f1a\u63d0\u793a\u53ef\u80fd\u5b58\u5728\u7684\u547d\u4ee4\r\n\r\n##### \u66fe\u7ecf\u4f7f\u7528\u8fc7\u7684\u547d\u4ee4\r\n\r\n- \u6309 \u4e0a/\u4e0b \u5149\u6807\u952e\u5207\u6362\r\n- \u4f7f\u7528`ctrl+c`\u9000\u51fa\u9009\u62e9\uff0c\u53e6\u8d77\u4e00\u884c\r\n\r\n\r\n\r\n\r\n\r\n##### 6\u4e2a\u5e38\u7528\u7684\u7ec8\u7aef\u547d\u4ee4\r\n\r\n```\r\nls\uff1a\uff08list\uff09\u67e5\u770b\u5f53\u524d\u6587\u4ef6\u5939\u4e0b\u7684\u5185\u5bb9\r\n\r\npwd\uff1a\uff08print work directoy\uff09\u67e5\u770b\u5f53\u524d\u6240\u5728\u6587\u4ef6\u5939\r\n\r\ncd\uff1a\uff08change directoy\uff09\u79fb\u52a8\u5230\u6478\u4e00\u4e2a\u6307\u5b9a\u6587\u4ef6\u5939\r\n\r\ntouch\uff1a\uff08touch\uff09\u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\uff0c\u65b0\u5efa\u6587\u4ef6\r\n\r\nmkdir\uff1a\uff08make dirctory\uff09\u521b\u5efa\u76ee\u5f55\r\n\r\nrm\uff1a\uff08remove\uff09\u5220\u9664\u6307\u5b9a\u6587\u4ef6\u540d\r\n\r\nclear\uff1a\uff08clear\uff09\u6e05\u5c4f\r\n```\r\n\r\n\r\n\r\n```\r\nctrl+shift+=\uff1a\u653e\u5927\u7ec8\u7aef\u7a97\u53e3\u7684\u5b57\u4f53\r\n\r\nctrl+-\uff1a\u7f29\u5c0f\r\n```\r\n\r\n\r\n\r\n##### ls\u547d\u4ee4\r\n\r\n```\r\nls       # \u4ec5\u5217\u51fa\u5f53\u524d\u76ee\u5f55\u53ef\u89c1\u6587\u4ef6\r\nls -l    # \u5217\u51fa\u5f53\u524d\u76ee\u5f55\u53ef\u89c1\u6587\u4ef6\u8be6\u7ec6\u4fe1\u606f\r\nls -hl   # \u5217\u51fa\u8be6\u7ec6\u4fe1\u606f\u5e76\u4ee5\u53ef\u8bfb\u5927\u5c0f\u663e\u793a\u6587\u4ef6\u5927\u5c0f\r\nls -al   # \u5217\u51fa\u6240\u6709\u6587\u4ef6\uff08\u5305\u62ec\u9690\u85cf\uff09\u7684\u8be6\u7ec6\u4fe1\u606f\r\nls --human-readable --size -1 -S --classify # \u6309\u6587\u4ef6\u5927\u5c0f\u6392\u5e8f\r\ndu -sh * | sort -h # \u6309\u6587\u4ef6\u5927\u5c0f\u6392\u5e8f(\u540c\u4e0a)\r\n```\r\n\r\n\r\n\r\n##### cd\u547d\u4ee4\r\n\r\n```\r\ncd    # \u8fdb\u5165\u7528\u6237\u4e3b\u76ee\u5f55\uff1b\r\ncd /  # \u8fdb\u5165\u6839\u76ee\u5f55\r\ncd ~  # \u8fdb\u5165\u7528\u6237\u4e3b\u76ee\u5f55\uff1b\r\ncd ..  # \u8fd4\u56de\u4e0a\u7ea7\u76ee\u5f55\uff08\u82e5\u5f53\u524d\u76ee\u5f55\u4e3a\u201c/\u201c\uff0c\u5219\u6267\u884c\u5b8c\u540e\u8fd8\u5728\u201c/'\uff1b'..'\u4e3a\u4e0a\u7ea7\u76ee\u5f55\u7684\u610f\u601d\uff09\uff1b\r\ncd ../..  # \u8fd4\u56de\u4e0a\u4e24\u7ea7\u76ee\u5f55\uff1b\r\ncd !$  # \u628a\u4e0a\u4e2a\u547d\u4ee4\u7684\u53c2\u6570\u4f5c\u4e3acd\u53c2\u6570\u4f7f\u7528\u3002", "top": 0, "createdAt": 1728523201, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-10-10", "dateLabelColor": "#bc4c00"}, "P12": {"htmlDir": "docs/post/apache-fan-xiang-dai-li-\uff08-bing-qi-yong-https\uff09.html", "labels": ["course"], "postTitle": "apache\u53cd\u5411\u4ee3\u7406\uff08\u5e76\u542f\u7528https\uff09", "postUrl": "post/apache-fan-xiang-dai-li-%EF%BC%88-bing-qi-yong-https%EF%BC%89.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/12", "commentNum": 0, "wordCount": 4411, "description": "\r\n\r\n```\r\n2024/11/15 \u66f4\u65b0\r\n\r\n\u53ef\u4ee5\u4f7f\u7528cloudfare\u542f\u7528https(\u8f83\u4e3a\u7b80\u5355\uff0c\u66f4\u65b0\u540e\u7684DNS\u53ef\u80fd\u9700\u89811\u5929\u5230\u4e24\u5929\u66f4\u65b0\u3002", "top": 0, "createdAt": 1729567565, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-10-22", "dateLabelColor": "#bc4c00"}, "P13": {"htmlDir": "docs/post/CDN-he-fan-xiang-dai-li-qian-jie.html", "labels": ["course"], "postTitle": "CDN\u548c\u53cd\u5411\u4ee3\u7406\u6d45\u89e3", "postUrl": "post/CDN-he-fan-xiang-dai-li-qian-jie.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/13", "commentNum": 0, "wordCount": 4504, "description": "#### CDN\r\n\r\nCDN\u7684\u5168\u79f0\u4e3a\u201cContent Delivery Network\u201d\uff0c\u53ca\uff0c\u5185\u5bb9\u5206\u53d1\u7f51\u7edc\u3002", "top": 0, "createdAt": 1729688097, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-10-23", "dateLabelColor": "#bc4c00"}, "P14": {"htmlDir": "docs/post/guan-yu-github-ke-long-cang-ku-chu-cuo-wen-ti.html", "labels": ["problem"], "postTitle": "\u5173\u4e8egithub\u514b\u9686\u4ed3\u5e93\u51fa\u9519\u95ee\u9898", "postUrl": "post/guan-yu-github-ke-long-cang-ku-chu-cuo-wen-ti.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/14", "commentNum": 0, "wordCount": 648, "description": "\u4f7f\u7528 git clone \u4e0b\u8f7d Github \u7b49\u7f51\u7ad9\u7684\u4ed3\u5e93\u65f6\uff0c\u53ef\u80fd\u4f1a\u9047\u5230\u7c7b\u4f3c 'Recv failure: Connection was reset' \u6216 'Failed to connect to http://github.com port 443 after 21114 ms: Couldn't connect to server' \u7684\u62a5\u9519\u3002", "top": 0, "createdAt": 1730032401, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-10-27", "dateLabelColor": "#bc4c00"}, "P15": {"htmlDir": "docs/post/websocket.html", "labels": ["course"], "postTitle": "websocket", "postUrl": "post/websocket.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/15", "commentNum": 0, "wordCount": 4315, "description": "<html><body>\r\n<!--StartFragment--><p>WebSocket \u662f\u4e00\u79cd\u7f51\u7edc\u901a\u4fe1\u534f\u8bae\uff0c\u65e8\u5728\u901a\u8fc7\u6301\u4e45\u7684\u3001\u5168\u53cc\u5de5\uff08\u53cc\u5411\uff09\u901a\u4fe1\u8fde\u63a5\u5b9e\u73b0\u5b9e\u65f6\u6570\u636e\u4ea4\u6362\u3002", "top": 0, "createdAt": 1731330901, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2024-11-11", "dateLabelColor": "#bc4c00"}, "P18": {"htmlDir": "docs/post/da-bao-python.html", "labels": ["course"], "postTitle": "\u6253\u5305python", "postUrl": "post/da-bao-python.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/18", "commentNum": 0, "wordCount": 4128, "description": "##### \u5c06python\u6253\u5305\u6210\u53ef\u6267\u884cexe\n\n\u5c06python\u6253\u5305\u7684\u65b9\u5f0f\u5927\u6982\u5206\u4e3a2\u79cd\uff0c\u5176\u4e2d\u6bcf\u4e00\u79cd\u90fd\u53ef\u4ee5\u7b80\u5355\u6253\u5305\u548c\u538b\u7f29\u6253\u5305\uff08\u538b\u7f29\u6253\u5305\u5c31\u662f\u6784\u4ef6\u5355\u72ec\u7684\u73af\u5883\uff0c\u5c06\u9879\u76ee\u4e0d\u9700\u8981\u7684\u5305\u9694\u79bb\u51fa\u53bb\uff09\n\n1. \u5355\u4e2a\u6587\u4ef6\u7684\u6253\u5305\n2. \u591a\u4e2a\u6587\u4ef6\u7684\u6253\u5305\uff08\u5f53\u9762\u5bf9\u4e00\u4e2a\u5927\u9879\u76ee\u7684\u65f6\u5019\uff0c\u4e3a\u4e86\u65b9\u4fbf\u7ef4\u62a4\uff0c\u901a\u5e38\u5c06\u4ee3\u7801\u5206\u5230\u4e0d\u540c\u7684\u6587\u4ef6\u4e2d\u3002", "top": 0, "createdAt": 1735804797, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-01-02", "dateLabelColor": "#0969da"}, "P19": {"htmlDir": "docs/post/I2C.html", "labels": ["course"], "postTitle": "I2C", "postUrl": "post/I2C.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/19", "commentNum": 0, "wordCount": 5292, "description": "I2C \u662f\u4e00\u79cd\u4e32\u884c\u901a\u4fe1\u534f\u8bae\u3002", "top": 0, "createdAt": 1735823349, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-01-02", "dateLabelColor": "#0969da"}, "P20": {"htmlDir": "docs/post/esp8266-lian-jie-si-you-fu-wu-qi.html", "labels": ["course"], "postTitle": "esp8266\u8fde\u63a5\u79c1\u6709\u670d\u52a1\u5668", "postUrl": "post/esp8266-lian-jie-si-you-fu-wu-qi.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/20", "commentNum": 0, "wordCount": 11185, "description": "ESP-01S \u57fa\u672c\u53c2\u6570\r\n![img](https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/222a8a308874efa976dbd68125140a57.png)\r\n\r\n![img](https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/ab7b3ba25b06f9e259135eb4b087300d.png)\r\n\r\n\u73af\u5883\uff1awin11 \uff0cArduous\u7248\u672c2.3.4   \uff0c ESP01S \uff0cUSB to TTL\r\n\r\n1\uff0c\u5b89\u88c5\u597dArduous IDE\uff08\u8fd9\u4e2a\u6ca1\u4ec0\u4e48\u597d\u8bf4\u7684\uff0c\u4fee\u6539\u4e00\u4e0b\u5b89\u88c5\u8def\u5f84\u3002", "top": 0, "createdAt": 1735827717, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-01-02", "dateLabelColor": "#0969da"}, "P21": {"htmlDir": "docs/post/vscode-yun-xing-c++.html", "labels": ["course"], "postTitle": "vscode\u8fd0\u884cc++", "postUrl": "post/vscode-yun-xing-c%2B%2B.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/21", "commentNum": 0, "wordCount": 3124, "description": "#### vscode\u8fd0\u884cc++\n\n###### \u5728c++\u7684\u7f16\u8bd1\u4e2d\uff0cg++\uff0cgdb\uff0cgcc\u90fd\u662f\u4ec0\u4e48\uff1f\n\n**GCC (GNU Compiler Collection)**\uff0c\u8fd9\u662fGNU\u9879\u76ee\u7684\u7f16\u8bd1\u5668\u96c6\u5408\uff0c\u6700\u521d\u662f'GNU C Compiler'\u7684\u7f29\u5199\uff0c\u73b0\u5728\u652f\u6301\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c**\u4e3b\u8981\u7528\u4e8e\u7f16\u8bd1C\u8bed\u8a00\u7a0b\u5e8f**\uff0c\u547d\u4ee4\u683c\u5f0f\uff1agcc source.c -o output\n\n**G++ (GNU C++ Compiler)**\u662fGCC\u7684\u4e00\u90e8\u5206\uff0c**\u4e13\u95e8\u7528\u4e8e\u7f16\u8bd1C++\u7a0b\u5e8f**\uff0c\u4f1a\u81ea\u52a8\u94fe\u63a5C++\u6807\u51c6\u5e93\uff0c\u5c06.cpp\u6587\u4ef6\u89c6\u4e3aC++\u6e90\u4ee3\u7801\uff08\u800cgcc\u9ed8\u8ba4\u5c06\u5176\u89c6\u4e3aC\u6587\u4ef6\uff09\uff0c\u547d\u4ee4\u683c\u5f0f\uff1ag++ source.cpp -o output\n\n**GDB (GNU Debugger)**\u662fGNU\u9879\u76ee\u7684\u8c03\u8bd5\u5668\uff0c\u7528\u4e8e\u7a0b\u5e8f\u8c03\u8bd5\uff0c\u53ef\u4ee5\uff1a\uff08\u8bbe\u7f6e\u65ad\u70b9\uff0c\u5355\u6b65\u6267\u884c\uff0c\u67e5\u770b\u53d8\u91cf\u503c\uff0c\u67e5\u770b\u8c03\u7528\u6808\uff0c\u76d1\u63a7\u53d8\u91cf\u53d8\u5316\uff09\u547d\u4ee4\u683c\u5f0f\uff1agdb ./program\n\nGNU\u9879\u76ee\uff08GNU Project\uff09\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u81ea\u7531\u8f6f\u4ef6\u8fd0\u52a8\u3002", "top": 0, "createdAt": 1739270538, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-02-11", "dateLabelColor": "#0969da"}, "P22": {"htmlDir": "docs/post/FreeRTOS_1.html", "labels": ["course"], "postTitle": "FreeRTOS_1", "postUrl": "post/FreeRTOS_1.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/22", "commentNum": 0, "wordCount": 32431, "description": "\n## \u76ee\u5f55\n\n<details>\n<summary>\u6982\u8ff0</summary>\n\n- [\u6982\u8ff0](#overview)\n</details>\n\n<details>\n<summary>1.\u57fa\u7840\u77e5\u8bc6</summary>\n\n- [1.\u57fa\u7840\u77e5\u8bc6](#basic-knowledge)\n  - [\u4e00.\u4efb\u52a1\u8c03\u5ea6\u5668\u7b80\u8ff0](#task-scheduler-overview)\n    - [1.\u4ec0\u4e48\u662f\u4efb\u52a1\u8c03\u5ea6\u5668](#what-is-task-scheduler)\n    - [2.freertos\u7684\u8c03\u5ea6\u65b9\u5f0f](#freertos-scheduling-methods)\n    - [3.\u62a2\u5360\u5f0f\u8c03\u5ea6\u8fc7\u7a0b](#preemptive-scheduling-process)\n    - [4.\u65f6\u95f4\u7247\u662f\u4ec0\u4e48](#what-is-time-slice)\n    - [5.\u65f6\u95f4\u7247\u8c03\u5ea6\u8fc7\u7a0b](#time-slice-scheduling-process)\n  - [\u4e8c.\u4efb\u52a1\u72b6\u6001](#task-states)\n    - [1.freertos\u7684\u4efb\u52a1\u72b6\u6001](#freertos-task-states)\n    - [2.\u56db\u79cd\u72b6\u6001\u4e4b\u95f4\u7684\u8f6c\u6362\u5173\u7cfb](#task-state-transitions)\n    - [3.\u4efb\u52a1\u72b6\u6001\u5217\u8868](#task-state-list)\n</details>\n\n<details>\n<summary>2.freertos\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6\u8be6\u89e3</summary>\n\n- [2.freertos\u7cfb\u7edf\u914d\u7f6e\u6587\u4ef6\u8be6\u89e3](#freertos-config-details)\n</details>\n\n<details>\n<summary>3.\u4efb\u52a1\u7684\u521b\u5efa\u548c\u5220\u9664</summary>\n\n- [3.\u4efb\u52a1\u7684\u521b\u5efa\u548c\u5220\u9664](#task-creation-and-deletion)\n  - [\u4e00.\u4efb\u52a1\u521b\u5efa\u548c\u5220\u9664API\u51fd\u6570](#task-creation-deletion-api)\n    - [1.\u4efb\u52a1\u521b\u5efa\u548c\u5220\u9664\u7684\u672c\u8d28](#task-creation-deletion-essence)\n    - [2.\u4efb\u52a1\u52a8\u6001\u521b\u5efa\u548c\u9759\u6001\u521b\u5efa\u7684\u533a\u522b](#dynamic-vs-static-task-creation)\n    - [3.\u4efb\u52a1\u63a7\u5236\u5757\u7ed3\u6784\u4f53\u6210\u5458\u4ecb\u7ecd](#task-control-block-members)\n    - [4.\u4ec0\u4e48\u662f\u4e34\u754c\u4fdd\u62a4\u533a](#what-is-critical-section)\n    - [5.\u52a8\u6001\u521b\u5efa\u7684\u4f18\u70b9](#dynamic-creation-advantages)\n    - [6.\u9759\u6001\u521b\u5efa\u7684\u4f18\u70b9](#static-creation-advantages)\n  - [\u4e8c.\u4efb\u52a1\u7684\u521b\u5efa\uff08\u52a8\u6001\uff09](#dynamic-task-creation)\n    - [1.\u52a8\u6001\u51fd\u6570\u7684\u521b\u5efa](#dynamic-function-creation)\n    - [2.\u4ec0\u4e48\u662f\u53e5\u67c4](#what-is-handle)\n    - [3.\u5b9e\u73b0\u52a8\u6001\u521b\u5efa\u4efb\u52a1\u6d41\u7a0b](#dynamic-task-creation-process)\n    - [4.\u52a8\u6001\u4efb\u52a1\u521b\u5efa\u51fd\u6570\u5185\u90e8\u5b9e\u73b0\u7b80\u8ff0](#dynamic-task-creation-internal)\n  - [\u4e09.\u4efb\u52a1\u7684\u521b\u5efa\uff08\u9759\u6001\uff09](#static-task-creation)\n    - [1.\u9759\u6001\u51fd\u6570\u7684\u521b\u5efa](#static-function-creation)\n    - [2.\u5b9e\u73b0\u9759\u6001\u521b\u5efa\u4efb\u52a1\u6d41\u7a0b](#static-task-creation-process)\n    - [3.\u9759\u6001\u4efb\u52a1\u521b\u5efa\u51fd\u6570\u5185\u90e8\u5b9e\u73b0\u7b80\u8ff0](#static-task-creation-internal)\n  - [\u56db.\u4efb\u52a1\u7684\u5220\u9664](#task-deletion)\n    - [1.\u4efb\u52a1\u5220\u9664\u51fd\u6570](#task-deletion-function)\n    - [2.\u5220\u9664\u4efb\u52a1\u6d41\u7a0b](#task-deletion-process)\n    - [3.\u5220\u9664\u4efb\u52a1\u51fd\u6570\u5185\u90e8\u5b9e\u73b0\u7b80\u8ff0](#task-deletion-internal)\n</details>\n\n<details>\n<summary>4.\u4efb\u52a1\u7684\u6302\u8d77\u548c\u6062\u590d</summary>\n\n- [4.\u4efb\u52a1\u7684\u6302\u8d77\u548c\u6062\u590d](#task-suspension-and-resumption)\n  - [\u4e00.\u4efb\u52a1\u7684\u6302\u8d77\u548c\u6062\u590d\u4ecb\u7ecd](#task-suspension-resumption-intro)\n  - [\u4e8c.\u4efb\u52a1\u7684\u6302\u8d77](#task-suspension)\n    - [1.\u6302\u8d77\u51fd\u6570\u4ecb\u7ecd](#suspension-function-intro)\n    - [2.\u4efb\u52a1\u6302\u8d77\u51fd\u6570\u5185\u90e8\u5b9e\u73b0](#suspension-function-internal)\n  - [\u4e09.\u4efb\u52a1\u7684\u6062\u590d](#task-resumption)\n    - [1.\u4efb\u52a1\u6062\u590d\u51fd\u6570\u4ecb\u7ecd\uff08\u4efb\u52a1\u4e2d\uff09](#resumption-function-task-intro)\n    - [2.\u4efb\u52a1\u56de\u590d\u51fd\u6570\u7684\u5b9e\u73b0\uff08\u4efb\u52a1\u4e2d\uff09](#resumption-function-task-internal)\n    - [3.\u4efb\u52a1\u6062\u590d\u51fd\u6570\u4ecb\u7ecd\uff08\u4e2d\u65ad\u4e2d\uff09](#resumption-function-isr-intro)\n    - [4.\u4efb\u52a1\u6062\u590d\u51fd\u6570\u5185\u90e8\u5b9e\u73b0\uff08\u4e2d\u65ad\u4e2d\uff09](#resumption-function-isr-internal)\n</details>\n\n<details>\n<summary>5.\u4e2d\u65ad\u7ba1\u7406</summary>\n\n- [5.\u4e2d\u65ad\u7ba1\u7406](#interrupt-management)\n  - [\u4e00.\u4e2d\u65ad\u4ecb\u7ecd](#interrupt-intro)\n    - [1.\u4ec0\u4e48\u662f\u4e2d\u65ad](#what-is-interrupt)\n    - [2.\u4e2d\u65ad\u6267\u884c\u673a\u5236](#interrupt-execution-mechanism)\n  - [\u4e8c.\u4e2d\u65ad\u4f18\u5148\u7ea7\u5206\u7ec4\u8bbe\u7f6e](#interrupt-priority-grouping)\n    - [1.\u4e2d\u65ad\u4f18\u5148\u7ea7\u5206\u7ec4\u4ecb\u7ecd](#priority-grouping-intro)\n    - [2.\u4ec0\u4e48\u662f\u53bb\u62a2\u5360\u4f18\u5148\u7ea7\u4ec0\u4e48\u662f\u5b50\u4f18\u5148\u7ea7](#preemption-vs-subpriority)\n    - [3.\u4e2d\u65ad\u4f18\u5148\u7ea7\u914d\u7f6e\u65b9\u5f0f](#priority-configuration-methods)\n    - [4.freertos\u4e2d\u5bf9\u4e2d\u65ad\u4f18\u5148\u7ea7\u7684\u7ba1\u7406](#freertos-interrupt-priority-management)\n  - [\u4e09.\u4e2d\u65ad\u76f8\u5173\u5bc4\u5b58\u5668](#interrupt-related-registers)\n    - [1.\u7cfb\u7edf\u4e2d\u65ad\u4f18\u5148\u7ea7\u914d\u7f6e\u5bc4\u5b58\u5668](#system-interrupt-priority-registers)\n    - [2.FreeRTOS\u5982\u4f55\u914d\u7f6ePendSV\u548cSystick\u4e2d\u65ad\u4f18\u5148\u7ea7](#freertos-pendsv-systick-config)\n    - [3.\u4e3a\u4ec0\u4e48\u5c06PendSV\u548cSysTick\u8bbe\u7f6e\u6700\u4f4e\u4f18\u5148\u7ea7](#why-lowest-pendsv-systick)\n    - [4.\u4e2d\u65ad\u5c4f\u853d\u5bc4\u5b58\u5668](#interrupt-mask-registers)\n    - [5.BASEPRI\u4e2d\u65ad\u5c4f\u853d\u5bc4\u5b58\u5668](#basepri-interrupt-mask)\n    - [6.freertos\u7684\u5173\u95ed\u4e2d\u65ad\u7a0b\u5e8f](#freertos-disable-interrupts)\n    - [7.freertos\u7684\u5f00\u4e2d\u65ad\u7a0b\u5e8f](#freertos-enable-interrupts)\n    - [8.\u4e2d\u65ad\u670d\u52a1\u51fd\u6570\u8c03\u7528FreeRTOS\u7684API\u51fd\u6570\u9700\u6ce8\u610f](#freertos-isr-api-notes)\n</details>\n\n<details>\n<summary>6.freertos\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4</summary>\n\n- [6.freertos\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4](#freertos-critical-section-protection)\n  - [1.\u4ec0\u4e48\u662f\u4e34\u754c\u6bb5](#what-is-critical-section-1)\n  - [2.\u9002\u7528\u4ec0\u4e48\u573a\u5408](#critical-section-use-cases)\n  - [3.\u4ec0\u4e48\u53ef\u4ee5\u6253\u65ad\u5f53\u524d\u7a0b\u5e8f\u7684\u8fd0\u884c](#what-interrupts-program)\n  - [4.\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4\u51fd\u6570](#critical-section-protection-functions)\n  - [5.\u4e34\u754c\u6bb5\u4ee3\u7801\u4fdd\u62a4\u51fd\u6570\u4f7f\u7528\u7279\u70b9](#critical-section-function-features)\n</details>\n\n<details>\n<summary>7.\u4efb\u52a1\u8c03\u5ea6\u5668\u6302\u8d77\u548c\u6062\u590d\u51fd\u6570</summary>\n\n- [7.\u4efb\u52a1\u8c03\u5ea6\u5668\u6302\u8d77\u548c\u6062\u590d\u51fd\u6570](#scheduler-suspend-resume-functions)\n  - [1.\u4efb\u52a1\u8c03\u5ea6\u5668\u6302\u8d77\u548c\u6062\u590d\u51fd\u6570](#scheduler-suspend-resume-functions-1)\n  - [2.\u4efb\u52a1\u8c03\u5ea6\u5668\u6302\u8d77\u548c\u6062\u590d\u7684\u7279\u70b9](#scheduler-suspend-resume-features)\n  - [3.\u6302\u8d77\u4efb\u52a1\u8c03\u5ea6\u5668vTaskSuspendAll](#suspend-scheduler-vtasksuspendall)\n  - [4.\u6062\u590d\u4efb\u52a1\u8c03\u5ea6\u5668xTaskResumeAll](#resume-scheduler-xtaskresumeall)\n</details>\n\n<details>\n<summary>8.freertos\u7684\u5217\u8868\u548c\u5217\u8868\u9879</summary>\n\n- [8.freertos\u7684\u5217\u8868\u548c\u5217\u8868\u9879](#freertos-lists-and-items)\n  - [\u4e00.\u5217\u8868\u548c\u5217\u8868\u9879\u7684\u7b80\u4ecb](#list-and-item-intro)\n    - [1.\u4ec0\u4e48\u662f\u5217\u8868](#what-is-list)\n    - [2.\u4ec0\u4e48\u662f\u5217\u8868\u9879](#what-is-list-item)\n    - [3.\u5217\u8868\u548c\u5217\u8868\u9879\u7684\u5173\u7cfb](#list-and-item-relationship)\n    - [4.\u5217\u8868\u94fe\u8868\u548c\u6570\u7ec4\u7684\u533a\u522b](#list-vs-array)\n    - [5.OS\u4e2d\u4e3a\u4ec0\u4e48\u4f7f\u7528\u5217\u8868](#why-use-lists-in-os)\n    - [6.\u5217\u8868\u7ed3\u6784\u4f53\u4ecb\u7ecd](#list-structure-intro)\n    - [7.\u5217\u8868\u9879\u7ed3\u6784\u4f53\u4ecb\u7ecd](#list-item-structure-intro)\n    - [8.\u8ff7\u4f60\u5217\u8868\u9879](#mini-list-item)\n    - [9.\u5217\u8868\u548c\u5217\u8868\u9879\u5173\u7cfb\u4e8b\u4f8b](#list-and-item-example)\n  - [\u4e8c.\u5217\u8868\u76f8\u5173\u7684API\u51fd\u6570\u4ecb\u7ecd](#list-related-api-intro)\n    - [1.\u5217\u8868API\u51fd\u6570](#list-api-functions)\n    - [2.\u521d\u59cb\u5316\u5217\u8868\u51fd\u6570vListInitialise](#init-list-vlistinitialise)\n    - [3.\u521d\u59cb\u5316\u5217\u8868\u9879\u51fd\u6570vListInitialiseItem](#init-list-item-vlistinitialiseitem)\n    - [4.\u5217\u8868\u63d2\u5165\u5217\u8868\u9879\u51fd\u6570vListInsert](#insert-list-vlistinsert)\n    - [5.\u5217\u8868\u672b\u5c3e\u63d2\u5165\u5217\u8868\u9879vListInsertEnd](#insert-end-vlistinsertend)\n    - [6.\u5217\u8868\u9879\u79fb\u9664\u51fd\u6570uxListRemove](#remove-list-uxlistremove)\n</details>\n\n<details>\n<summary>9.freertos\u4efb\u52a1\u8c03\u5ea6</summary>\n\n- [9.freertos\u4efb\u52a1\u8c03\u5ea6](#freertos-task-scheduling)\n  - [\u4e00.\u5f00\u542f\u4efb\u52a1\u8c03\u5ea6\u5668\u719f\u6089](#start-scheduler-overview)\n    - [1.\u5f00\u542f\u4efb\u52a1\u8c03\u5ea6\u5668\u51fd\u6570vTaskStartScheduler](#start-scheduler-vtaskstartscheduler)\n    - [2.\u914d\u7f6e\u786c\u4ef6\u67b6\u6784\u53ca\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1\u51fd\u6570xPortStartScheduler](#config-hardware-xportstartscheduler)\n    - [3.SysTick\u6ef4\u7b54\u5b9a\u65f6\u5668](#systick-timer)\n    - [4.\u5806\u548c\u6808\u7684\u5730\u5740\u751f\u957f\u65b9\u5411](#heap-stack-growth)\n    - [5.\u538b\u6808\u548c\u51fa\u6808\u7684\u5730\u5740\u589e\u957f\u65b9\u5411](#stack-push-pop-direction)\n    - [6.\u77e5\u8bc6\u8865\u5145](#knowledge-supplement)\n  - [\u4e8c.\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1\u719f\u6089](#start-first-task-overview)\n    - [1.\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1\u6d89\u53ca\u7684\u5173\u952e\u51fd\u6570](#start-first-task-key-functions)\n    - [2.\u60f3\u8c61\u4e00\u4e0b\u5e94\u8be5\u5982\u4f55\u542f\u52a8\u7b2c\u4e00\u4e2a\u4efb\u52a1](#how-to-start-first-task)\n    - [3.prvStartFirstTask \u4ecb\u7ecd](#prvstartfirsttask-intro)\n    - [4.\u4ec0\u4e48\u662fMSP\u6307\u9488](#what-is-msp-pointer)\n    - [5.\u4e3a\u4ec0\u4e48\u6c47\u7f16\u4ee3\u7801\u8981PRESERVE8\u516b\u5b57\u8282\u5bf9\u9f50](#why-preserve8-alignment)\n    - [6.prvStartFirstTask\u4e3a\u4ec0\u4e48\u8981\u64cd\u4f5c0XE00ED08](#prvstartfirsttask-0xe00ed08)\n    - [7.vPortSVCHandle\u4ecb\u7ecd](#vportsvchandle-intro)\n    - [8.\u51fa\u6808\u538b\u6808\u6c47\u7f16\u6307\u4ee4\u8be6\u89e3](#stack-instruction-details)\n  - [\u4e09.\u4efb\u52a1\u5207\u6362\u638c\u63e1](#task-switching-mastery)\n    - [1.\u4efb\u52a1\u5207\u6362\u7684\u672c\u8d28](#task-switching-essence)\n    - [2.\u4efb\u52a1\u5207\u6362\u8fc7\u7a0b](#task-switching-process)\n    - [3.PendSV\u4e2d\u65ad\u662f\u5982\u4f55\u89e6\u53d1\u7684](#pendsv-trigger)\n    - [4.\u5728PendSV\u4e2d\u65ad\u4e2dPSP\u548cMSP](#pendsv-psp-msp)\n    - [5.\u67e5\u627e\u6700\u9ad8\u4f18\u5148\u7ea7\u4efb\u52a1](#find-highest-priority-task)\n    - [6.\u524d\u5bfc\u7f6e\u96f6\u6307\u4ee4](#leading-zero-instruction)\n</details>\n\n<details>\n<summary>10.FreeRTOS\u65f6\u95f4\u7247\u8f6e\u8be2</summary>\n\n- [10.FreeRTOS\u65f6\u95f4\u7247\u8f6e\u8be2](#freertos-timeslice-polling)\n  - [\u4e00.\u65f6\u95f4\u7247\u8f6e\u8be2\u7b80\u4ecb](#timeslice-polling-intro)\n</details>\n\n---\n\n## <a id='freertos'></a>FreeRTOS\n\n## <a id='overview'></a>\u6982\u8ff0\n\n\u968f\u7740\u4ea7\u54c1\u5b9e\u73b0\u7684\u529f\u80fd\u8d8a\u6765\u8d8a\u591a\uff0c\u5355\u7eaf\u7684\u88f8\u673a\u7cfb\u7edf\u5df2\u7ecf\u4e0d\u80fd\u5b8c\u7f8e\u7684\u89e3\u51b3\u95ee\u9898\u4e86\uff0c\u53cd\u800c\u4f1a\u4f7f\u7a0b\u5e8f\u8fb9\u7684\u66f4\u52a0\u590d\u6742\uff0c\u5982\u679c\u60f3\u964d\u4f4e\u7f16\u7a0b\u7684\u96be\u5ea6\uff0c\u6211\u4eec\u53ef\u4ee5\u8003\u8651\u5f15\u5165RTOS\u5b9e\u73b0\u591a\u4efb\u52a1\u7ba1\u7406\u3002", "top": 0, "createdAt": 1740907019, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-03-02", "dateLabelColor": "#0969da"}, "P24": {"htmlDir": "docs/post/FreeRTOS_2.html", "labels": ["course"], "postTitle": "FreeRTOS_2", "postUrl": "post/FreeRTOS_2.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/24", "commentNum": 0, "wordCount": 51146, "description": "\n[TOC]\n\n## 11.FreeRTOS\u4efb\u52a1\u76f8\u5173\u7684\u5176\u4ed6API\u51fd\u6570\n\n### \u4e00\u3001FreeRTOS\u4efb\u52a1\u76f8\u5173\u7684\u5176\u4ed6API\u51fd\u6570\u4ecb\u7ecd\n\n#### 1\u3001FreeRTOS\u4efb\u52a1\u76f8\u5173API\u51fd\u6570\u4ecb\u7ecd(\u90e8\u5206\u5e38\u7528\u7684)\n\n\u7b54\uff1a\n\n![](https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/freertos/%E4%BB%BB%E5%8A%A1%E7%9B%B8%E5%85%B3%E7%9A%84%E5%85%B6%E4%BB%96API%E5%87%BD%E6%95%B0.png)\n\n### \u4e8c\u3001\u4efb\u52a1\u72b6\u6001\u67e5\u8be2API\u51fd\u6570\n\n#### 1\u3001\u83b7\u53d6\u4efb\u52a1\u4f18\u5148\u7ea7\u51fd\u6570\n\n\u7b54\uff1a\n\n```C\nUBaseType_t  uxTaskPriorityGet(  const TaskHandle_t xTask  )\n```\n\n\u6b64\u51fd\u6570\u7528\u4e8e\u83b7\u53d6\u6307\u5b9a\u4efb\u52a1\u7684\u4efb\u52a1\u4f18\u5148\u7ea7\uff0c\u4f7f\u7528\u8be5\u51fd\u6570\u9700\u8981\u5c06\u5b8f INCLUDE_uxTaskPriorityGet \u7f6e1\u3002", "top": 0, "createdAt": 1741154679, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-03-05", "dateLabelColor": "#0969da"}, "P25": {"htmlDir": "docs/post/vscode-shi-xian-yuan-cheng-kai-fa.html", "labels": ["course"], "postTitle": "vscode\u5b9e\u73b0\u8fdc\u7a0b\u5f00\u53d1", "postUrl": "post/vscode-shi-xian-yuan-cheng-kai-fa.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/25", "commentNum": 0, "wordCount": 1134, "description": "##### \u539f\u7406\uff1a\n\nVSCode \u7684 Remote - SSH \u529f\u80fd\u672c\u8d28\u4e0a\u662f\u5229\u7528 SSH \u534f\u8bae\uff0c\u5728\u672c\u5730\u673a\u5668\uff08\u5ba2\u6237\u7aef\uff09\u548c\u8fdc\u7a0b\u4e91\u670d\u52a1\u5668\u4e4b\u95f4\u5efa\u7acb\u5b89\u5168\u8fde\u63a5\uff0c\u7136\u540e\u5728\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\u8fd0\u884c\u4e00\u4e2a VSCode Server \u5b9e\u4f8b\u3002", "top": 0, "createdAt": 1741689232, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-03-11", "dateLabelColor": "#0969da"}, "P26": {"htmlDir": "docs/post/kc-unet++-yuan-ma.html", "labels": ["course"], "postTitle": "kc-unet++\u6e90\u7801", "postUrl": "post/kc-unet%2B%2B-yuan-ma.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/26", "commentNum": 0, "wordCount": 50820, "description": "train.py\n```\nimport argparse\nimport os\nfrom collections import OrderedDict\nfrom glob import glob\n\nimport pandas as pd\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nimport albumentations as A\nfrom albumentations.core.composition import Compose, OneOf\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\n\nimport archs\nimport losses\nfrom dataset import Dataset\nfrom metrics import iou_score, dice_coef, precision_score, recall_score\nfrom utils import AverageMeter, str2bool\n\nARCH_NAMES = archs.__all__\nLOSS_NAMES = losses.__all__\nLOSS_NAMES.append('BCEWithLogitsLoss')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--name', default=None,\n                        help='model name: (default: arch+timestamp)')\n    parser.add_argument('--epochs', default=150, type=int, metavar='N',\n                        help='number of total epochs to run')\n    parser.add_argument('-b', '--batch_size', default=8, type=int,\n                        metavar='N', help='mini-batch size (default: 16)')\n\n    # model\n    parser.add_argument('--arch', '-a', metavar='ARCH', default='UKAN_NestedUNet',\n                        choices=ARCH_NAMES,\n                        help='model architecture: ' +\n                             ' | '.join(ARCH_NAMES) +\n                             ' (default: NestedUNet)')\n    parser.add_argument('--deep_supervision', default=False, type=str2bool)\n    parser.add_argument('--input_channels', default=3, type=int,\n                        help='input channels')\n    parser.add_argument('--num_classes', default=1, type=int,\n                        help='number of classes')\n    parser.add_argument('--input_w', default=512, type=int,\n                        help='image width')\n    parser.add_argument('--input_h', default=512, type=int,\n                        help='image height')\n\n    # loss\n    parser.add_argument('--loss', default='BCEDiceLoss',\n                        choices=LOSS_NAMES,\n                        help='loss: ' +\n                             ' | '.join(LOSS_NAMES) +\n                             ' (default: BCEDiceLoss)')\n\n    # dataset\n    parser.add_argument('--dataset', default='dsb2018_96_1000',\n                        help='dataset name')\n    parser.add_argument('--img_ext', default='.jpg',\n                        help='image file extension')\n    parser.add_argument('--mask_ext', default='.png',\n                        help='mask file extension')\n\n    # optimizer\n    parser.add_argument('--optimizer', default='SGD',\n                        choices=['Adam', 'SGD'],\n                        help='loss: ' +\n                             ' | '.join(['Adam', 'SGD']) +\n                             ' (default: Adam)')\n    parser.add_argument('--lr', '--learning_rate', default=1e-3, type=float,\n                        metavar='LR', help='initial learning rate')\n    parser.add_argument('--momentum', default=0.9, type=float,\n                        help='momentum')\n    parser.add_argument('--weight_decay', default=1e-4, type=float,\n                        help='weight decay')\n    parser.add_argument('--nesterov', default=False, type=str2bool,\n                        help='nesterov')\n\n    # scheduler\n    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n    parser.add_argument('--min_lr', default=1e-5, type=float,\n                        help='minimum learning rate')\n    parser.add_argument('--factor', default=0.1, type=float)\n    parser.add_argument('--patience', default=2, type=int)\n    parser.add_argument('--milestones', default='1,2', type=str)\n    parser.add_argument('--gamma', default=2 / 3, type=float)\n    parser.add_argument('--early_stopping', default=-1, type=int,\n                        metavar='N', help='early stopping (default: -1)')\n\n    parser.add_argument('--num_workers', default=4, type=int)\n\n    config = parser.parse_args()\n\n    return config\n\n\ndef train(config, train_loader, model, criterion, optimizer):\n    avg_meters = {'loss': AverageMeter(),\n                  'iou': AverageMeter(),\n                  'dice': AverageMeter(),\n                  'precision': AverageMeter(),\n                  'recall': AverageMeter()}\n\n    model.train()\n\n    pbar = tqdm(total=len(train_loader))\n    for input, target, _ in train_loader:\n        input = input.cuda()\n        target = target.cuda()\n\n        # compute output\n        if config['deep_supervision']:\n            outputs = model(input)\n            loss = 0\n            for output in outputs:\n                loss += criterion(output, target)\n            loss /= len(outputs)\n            output = outputs[-1]\n        else:\n            output = model(input)\n            loss = criterion(output, target)\n\n        # compute metrics\n        iou = iou_score(output, target)\n        dice = dice_coef(output, target)\n        precision = precision_score(output, target)\n        recall = recall_score(output, target)\n\n        # compute gradient and do optimizing step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        avg_meters['loss'].update(loss.item(), input.size(0))\n        avg_meters['iou'].update(iou, input.size(0))\n        avg_meters['dice'].update(dice, input.size(0))\n        avg_meters['precision'].update(precision, input.size(0))\n        avg_meters['recall'].update(recall, input.size(0))\n\n        postfix = OrderedDict([\n            ('loss', avg_meters['loss'].avg),\n            ('iou', avg_meters['iou'].avg),\n            ('dice', avg_meters['dice'].avg),\n            ('precision', avg_meters['precision'].avg),\n            ('recall', avg_meters['recall'].avg),\n        ])\n        pbar.set_postfix(postfix)\n        pbar.update(1)\n    pbar.close()\n\n    return OrderedDict([('loss', avg_meters['loss'].avg),\n                        ('iou', avg_meters['iou'].avg),\n                        ('dice', avg_meters['dice'].avg),\n                        ('precision', avg_meters['precision'].avg),\n                        ('recall', avg_meters['recall'].avg)])\n\n\ndef validate(config, val_loader, model, criterion):\n    avg_meters = {'loss': AverageMeter(),\n                  'iou': AverageMeter(),\n                  'dice': AverageMeter(),\n                  'precision': AverageMeter(),\n                  'recall': AverageMeter()}\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        pbar = tqdm(total=len(val_loader))\n        for input, target, _ in val_loader:\n            input = input.cuda()\n            target = target.cuda()\n\n            # compute output\n            if config['deep_supervision']:\n                outputs = model(input)\n                loss = 0\n                for output in outputs:\n                    loss += criterion(output, target)\n                loss /= len(outputs)\n                output = outputs[-1]\n            else:\n                output = model(input)\n                loss = criterion(output, target)\n\n            # compute metrics\n            iou = iou_score(output, target)\n            dice = dice_coef(output, target)\n            precision = precision_score(output, target)\n            recall = recall_score(output, target)\n\n            avg_meters['loss'].update(loss.item(), input.size(0))\n            avg_meters['iou'].update(iou, input.size(0))\n            avg_meters['dice'].update(dice, input.size(0))\n            avg_meters['precision'].update(precision, input.size(0))\n            avg_meters['recall'].update(recall, input.size(0))\n\n            postfix = OrderedDict([\n                ('loss', avg_meters['loss'].avg),\n                ('iou', avg_meters['iou'].avg),\n                ('dice', avg_meters['dice'].avg),\n                ('precision', avg_meters['precision'].avg),\n                ('recall', avg_meters['recall'].avg),\n            ])\n            pbar.set_postfix(postfix)\n            pbar.update(1)\n        pbar.close()\n\n    return OrderedDict([('loss', avg_meters['loss'].avg),\n                        ('iou', avg_meters['iou'].avg),\n                        ('dice', avg_meters['dice'].avg),\n                        ('precision', avg_meters['precision'].avg),\n                        ('recall', avg_meters['recall'].avg)])\n\n\ndef main():\n    config = vars(parse_args())\n\n    if config['name'] is None:\n        if config['deep_supervision']:\n            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])\n        else:\n            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])\n    os.makedirs('models/%s' % config['name'], exist_ok=True)\n\n    print('-' * 20)\n    for key in config:\n        print('%s: %s' % (key, config[key]))\n    print('-' * 20)\n\n    with open('models/%s/config.yml' % config['name'], 'w') as f:\n        yaml.dump(config, f)\n\n    # define loss function (criterion)\n    if config['loss'] == 'BCEWithLogitsLoss':\n        criterion = nn.BCEWithLogitsLoss().cuda()\n    else:\n        criterion = losses.__dict__[config['loss']]().cuda()\n\n    cudnn.benchmark = True\n\n    # create model\n    print('=> creating model %s' % config['arch'])\n    model = archs.__dict__[config['arch']](config['num_classes'],\n                                           config['input_channels'],\n                                           config['deep_supervision'])\n\n    # Enable multi-GPU support with DataParallel\n    if torch.cuda.device_count() > 1:\n        print(f'=> Using {torch.cuda.device_count()} GPUs!')\n        model = nn.DataParallel(model)\n    model = model.cuda()\n\n    params = filter(lambda p: p.requires_grad, model.parameters())\n    if config['optimizer'] == 'Adam':\n        optimizer = optim.Adam(\n            params, lr=config['lr'], weight_decay=config['weight_decay'])\n    elif config['optimizer'] == 'SGD':\n        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n    else:\n        raise NotImplementedError\n\n    if config['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n    elif config['scheduler'] == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],\n                                                   verbose=1, min_lr=config['min_lr'])\n    elif config['scheduler'] == 'MultiStepLR':\n        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')],\n                                             gamma=config['gamma'])\n    elif config['scheduler'] == 'ConstantLR':\n        scheduler = None\n    else:\n        raise NotImplementedError\n\n    # Data loading code\n    img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))\n    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n\n    train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n\n    train_transform = Compose([\n        A.RandomRotate90(),\n        A.HorizontalFlip(p=0.5),  # 50% \u6982\u7387\u6c34\u5e73\u7ffb\u8f6c\n        A.VerticalFlip(p=0.5),  # 50% \u6982\u7387\u5782\u76f4\u7ffb\u8f6c\n        OneOf([\n            A.HueSaturationValue(),\n            A.RandomBrightnessContrast(),\n        ], p=1),\n        A.Resize(config['input_h'], config['input_w']),\n        A.Normalize(),\n    ])\n\n    val_transform = Compose([\n        A.Resize(config['input_h'], config['input_w']),\n        A.Normalize(),\n    ])\n\n    train_dataset = Dataset(\n        img_ids=train_img_ids,\n        img_dir=os.path.join('inputs', config['dataset'], 'images'),\n        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=train_transform)\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join('inputs', config['dataset'], 'images'),\n        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=val_transform)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        drop_last=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        drop_last=False)\n\n    # \u521b\u5efaCSV\u6587\u4ef6\n    df = pd.DataFrame(columns=['epoch', 'loss', 'iou', 'dice', 'precision', 'recall',\n                              'val_loss', 'val_iou', 'val_dice', 'val_precision', 'val_recall'])\n    df.to_csv('models/%s/log.csv' % config['name'], index=False)\n\n    best_iou = 0\n    best_epoch = 0\n    for epoch in range(config['epochs']):\n        print('\\nEpoch [%d/%d]' % (epoch, config['epochs']))\n\n        # train for one epoch\n        train_log = train(config, train_loader, model, criterion, optimizer)\n        val_log = validate(config, val_loader, model, criterion)\n\n        if config['scheduler'] == 'CosineAnnealingLR':\n            scheduler.step()\n        elif config['scheduler'] == 'ReduceLROnPlateau':\n            scheduler.step(val_log['loss'])\n\n        print('loss %.4f - iou %.4f - dice %.4f - precision %.4f - recall %.4f - val_loss %.4f - val_iou %.4f - val_dice %.4f - val_precision %.4f - val_recall %.4f'\n              % (train_log['loss'], train_log['iou'], train_log['dice'], train_log['precision'], train_log['recall'],\n                 val_log['loss'], val_log['iou'], val_log['dice'], val_log['precision'], val_log['recall']))\n\n        df = pd.DataFrame([[epoch, train_log['loss'], train_log['iou'], train_log['dice'], train_log['precision'], train_log['recall'],\n                          val_log['loss'], val_log['iou'], val_log['dice'], val_log['precision'], val_log['recall']]],\n                         columns=['epoch', 'loss', 'iou', 'dice', 'precision', 'recall',\n                                 'val_loss', 'val_iou', 'val_dice', 'val_precision', 'val_recall'])\n        df.to_csv('models/%s/log.csv' % config['name'], mode='a', header=False, index=False)\n\n        if val_log['iou'] > best_iou:\n            print('=> saved best model')\n            best_iou = val_log['iou']\n            best_epoch = epoch\n            torch.save(model.state_dict(), 'models/%s/model.pth' % config['name'])\n\n    print('=> Best IoU: %.4f at epoch %d' % (best_iou, best_epoch))\n\n\nif __name__ == '__main__':\n    main()\n```\n\narchs.py\n```\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\nfrom kan import KANLinear\nfrom model.attention.CBAM import CBAMBlock  # \u5bfc\u5165CBAM\u6a21\u5757\n\n__all__ = ['UKAN_NestedUNet']\n\nclass KANLayer(nn.Module):\n    '''\n    KAN\u5c42\u5b9e\u73b0\uff0c\u57fa\u4e8eKolmogorov-Arnold\u7f51\u7edc\n    \u8fd9\u662f\u4e00\u4e2a\u7279\u6b8a\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\uff0c\u4f7f\u7528\u6837\u6761\u51fd\u6570\u6765\u589e\u5f3a\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\n    '''\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        # \u7f51\u683c\u5927\u5c0f\uff0c\u63a7\u5236\u6837\u6761\u51fd\u6570\u7684\u590d\u6742\u5ea6\n        grid_size = 3\n        # \u6837\u6761\u51fd\u6570\u7684\u9636\u6570\n        spline_order = 2\n        # \u566a\u58f0\u7f29\u653e\u56e0\u5b50\uff0c\u7528\u4e8e\u521d\u59cb\u5316\n        scale_noise = 0.1\n        # \u57fa\u7840\u7f29\u653e\u56e0\u5b50\n        scale_base = 1.0\n        # \u6837\u6761\u7f29\u653e\u56e0\u5b50\n        scale_spline = 1.0\n        # \u57fa\u7840\u6fc0\u6d3b\u51fd\u6570\n        base_activation = nn.SiLU\n        # \u7f51\u683cepsilon\u503c\uff0c\u9632\u6b62\u7f51\u683c\u70b9\u8fc7\u4e8e\u63a5\u8fd1\n        grid_eps = 0.02\n        # \u7f51\u683c\u8303\u56f4\n        grid_range = [-1, 1]\n\n        # \u7b2c\u4e00\u4e2aKAN\u7ebf\u6027\u5c42\uff0c\u5c06\u8f93\u5165\u7279\u5f81\u6620\u5c04\u5230\u9690\u85cf\u7279\u5f81\n        self.fc1 = KANLinear(\n            in_features,\n            hidden_features,\n            grid_size=grid_size,\n            spline_order=spline_order,\n            scale_noise=scale_noise,\n            scale_base=scale_base,\n            scale_spline=scale_spline,\n            base_activation=base_activation,\n            grid_eps=grid_eps,\n            grid_range=grid_range,\n        )\n        # \u7b2c\u4e8c\u4e2aKAN\u7ebf\u6027\u5c42\uff0c\u5c06\u9690\u85cf\u7279\u5f81\u6620\u5c04\u5230\u8f93\u51fa\u7279\u5f81\n        self.fc2 = KANLinear(\n            hidden_features,\n            out_features,\n            grid_size=grid_size,\n            spline_order=spline_order,\n            scale_noise=scale_noise,\n            scale_base=scale_base,\n            scale_spline=scale_spline,\n            base_activation=base_activation,\n            grid_eps=grid_eps,\n            grid_range=grid_range,\n        )\n        # \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff0c\u7528\u4e8e\u6355\u83b7\u7a7a\u95f4\u4fe1\u606f\n        self.dwconv = nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features)\n        # \u6279\u5f52\u4e00\u5316\u5c42\n        self.bn = nn.BatchNorm2d(hidden_features)\n        # ReLU\u6fc0\u6d3b\u51fd\u6570\n        self.relu = nn.ReLU()\n        # Dropout\u5c42\uff0c\u7528\u4e8e\u6b63\u5219\u5316\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x, H, W):\n        '''\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\n        x: \u8f93\u5165\u7279\u5f81 [B, N, C]\n        H, W: \u7279\u5f81\u56fe\u7684\u9ad8\u548c\u5bbd\n        '''\n        B, N, C = x.shape\n        # \u5e94\u7528\u7b2c\u4e00\u4e2aKAN\u7ebf\u6027\u5c42\n        x = self.fc1(x.reshape(B * N, C)).reshape(B, N, -1)\n        # \u91cd\u5851\u4e3a\u56fe\u50cf\u683c\u5f0f\u4ee5\u5e94\u7528\u5377\u79ef\n        x = x.transpose(1, 2).view(B, -1, H, W)\n        # \u5e94\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3001\u6279\u5f52\u4e00\u5316\u548cReLU\n        x = self.relu(self.bn(self.dwconv(x)))\n        # \u91cd\u5851\u56de\u5e8f\u5217\u683c\u5f0f\n        x = x.flatten(2).transpose(1, 2)\n        # \u5e94\u7528\u7b2c\u4e8c\u4e2aKAN\u7ebf\u6027\u5c42\n        x = self.fc2(x.reshape(B * N, -1)).reshape(B, N, -1)\n        # \u5e94\u7528dropout\u5e76\u8fd4\u56de\n        return self.drop(x)\n\nclass KANBlock(nn.Module):\n    '''\n    KAN\u5757\uff0c\u5305\u542b\u4e00\u4e2aLayerNorm\u548c\u4e00\u4e2aKANLayer\uff0c\u5e76\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\n    '''\n    def __init__(self, dim, drop=0., drop_path=0., norm_layer=nn.LayerNorm):\n        super().__init__()\n        # DropPath\u7528\u4e8e\u968f\u673a\u4e22\u5f03\u6b8b\u5dee\u8fde\u63a5\uff0c\u589e\u5f3a\u6b63\u5219\u5316\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        # \u5c42\u5f52\u4e00\u5316\n        self.norm = norm_layer(dim)\n        # KAN\u5c42\n        self.layer = KANLayer(in_features=dim, hidden_features=dim, drop=drop)\n\n    def forward(self, x, H, W):\n        '''\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\uff0c\u5b9e\u73b0\u6b8b\u5dee\u8fde\u63a5\n        '''\n        return x + self.drop_path(self.layer(self.norm(x), H, W))\n\nclass PatchEmbed(nn.Module):\n    '''\n    \u56fe\u50cf\u5757\u5d4c\u5165\u5c42\uff0c\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\n    '''\n    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        # \u8ba1\u7b97\u8f93\u51fa\u7279\u5f81\u56fe\u7684\u9ad8\u548c\u5bbd\n        self.H, self.W = img_size[0] // stride, img_size[1] // stride\n        # \u8ba1\u7b97\u56fe\u50cf\u5757\u7684\u6570\u91cf\n        self.num_patches = self.H * self.W\n        # \u5377\u79ef\u5c42\uff0c\u7528\u4e8e\u63d0\u53d6\u56fe\u50cf\u5757\u7279\u5f81\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n        # \u5c42\u5f52\u4e00\u5316\n        self.norm = nn.LayerNorm(embed_dim)\n\n    def forward(self, x):\n        '''\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\n        \u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\n        '''\n        # \u5e94\u7528\u5377\u79ef\n        x = self.proj(x)\n        # \u83b7\u53d6\u8f93\u51fa\u7279\u5f81\u56fe\u7684\u5c3a\u5bf8\n        _, _, H, W = x.shape\n        # \u5c06\u7279\u5f81\u56fe\u5c55\u5e73\u4e3a\u5e8f\u5217\n        x = x.flatten(2).transpose(1, 2)\n        # \u5e94\u7528\u5c42\u5f52\u4e00\u5316\n        x = self.norm(x)\n        return x, H, W\n\nclass VGGBlock(nn.Module):\n    '''\n    VGG\u5757\uff0c\u5305\u542b\u4e24\u4e2a\u5377\u79ef\u5c42\uff0c\u6bcf\u4e2a\u5377\u79ef\u5c42\u540e\u8ddf\u6279\u5f52\u4e00\u5316\u548cReLU\u6fc0\u6d3b\n    '''\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        # ReLU\u6fc0\u6d3b\u51fd\u6570\n        self.relu = nn.ReLU(inplace=True)\n        # \u7b2c\u4e00\u4e2a\u5377\u79ef\u5c42\n        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n        # \u7b2c\u4e00\u4e2a\u6279\u5f52\u4e00\u5316\u5c42\n        self.bn1 = nn.BatchNorm2d(middle_channels)\n        # \u7b2c\u4e8c\u4e2a\u5377\u79ef\u5c42\n        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n        # \u7b2c\u4e8c\u4e2a\u6279\u5f52\u4e00\u5316\u5c42\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        '''\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\n        '''\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        return out\n\nclass UKAN_NestedUNet(nn.Module):\n    '''\n    UKAN\u5d4c\u5957UNet\u6a21\u578b\uff0c\u5c06KAN\u6a21\u5757\u96c6\u6210\u5230UNet++\u67b6\u6784\u4e2d\n    KAN\u6a21\u5757\u4e3b\u8981\u5728\u6df1\u5c42\u7279\u5f81(x2_0, x2_2, x3_0, x4_0, x3_1)\u4e2d\u53d1\u6325\u4f5c\u7528\n    '''\n    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, **kwargs):\n        super().__init__()\n        # \u5b9a\u4e49\u6bcf\u5c42\u7684\u6ee4\u6ce2\u5668\u6570\u91cf\n        self.nb_filter = [32, 64, 128, 256, 512]\n        # \u662f\u5426\u4f7f\u7528\u6df1\u5ea6\u76d1\u7763\n        self.deep_supervision = deep_supervision\n        # \u6700\u5927\u6c60\u5316\u5c42\uff0c\u7528\u4e8e\u4e0b\u91c7\u6837\n        self.pool = nn.MaxPool2d(2, 2)\n        # \u4e0a\u91c7\u6837\u5c42\uff0c\u7528\u4e8e\u4e0a\u91c7\u6837\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        # \u7f16\u7801\u5668\u90e8\u5206\n        # \u7b2c\u4e00\u5c42\u7f16\u7801\u5668\n        self.conv0_0 = VGGBlock(input_channels, self.nb_filter[0], self.nb_filter[0])\n        # \u7b2c\u4e8c\u5c42\u7f16\u7801\u5668\n        self.conv1_0 = VGGBlock(self.nb_filter[0], self.nb_filter[1], self.nb_filter[1])\n        # \u7b2c\u4e09\u5c42\u7f16\u7801\u5668\n        self.conv2_0 = VGGBlock(self.nb_filter[1], self.nb_filter[2], self.nb_filter[2])\n        \n        # \u65b0\u589e\uff1a\u4e3ax2_0\u6dfb\u52a0PatchEmbed\u548cKANBlock\n        self.patch_embed2 = PatchEmbed(img_size=img_size//4, patch_size=3, stride=1, in_chans=self.nb_filter[2], embed_dim=self.nb_filter[2])\n        self.block2 = KANBlock(dim=self.nb_filter[2], norm_layer=nn.LayerNorm)\n        \n        # \u7b2c\u56db\u5c42\u7f16\u7801\u5668\n        self.conv3_0 = VGGBlock(self.nb_filter[2], self.nb_filter[3], self.nb_filter[3])\n        # \u7b2c\u56db\u5c42\u7684\u56fe\u50cf\u5757\u5d4c\u5165\uff0c\u5c06\u7279\u5f81\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\u4ee5\u5e94\u7528KAN\n        self.patch_embed3 = PatchEmbed(img_size=img_size//8, patch_size=3, stride=2, in_chans=self.nb_filter[3], embed_dim=self.nb_filter[3])\n        # \u7b2c\u56db\u5c42\u7684KAN\u5757\uff0c\u589e\u5f3a\u7279\u5f81\u8868\u8fbe\u80fd\u529b\n        self.block3 = KANBlock(dim=self.nb_filter[3], norm_layer=nn.LayerNorm)\n        # \u7b2c\u4e94\u5c42\u7684\u56fe\u50cf\u5757\u5d4c\u5165\n        self.patch_embed4 = PatchEmbed(img_size=img_size//16, patch_size=3, stride=2, in_chans=self.nb_filter[3], embed_dim=self.nb_filter[4])\n        # \u7b2c\u4e94\u5c42\u7684KAN\u5757\n        self.block4 = KANBlock(dim=self.nb_filter[4], norm_layer=nn.LayerNorm)\n\n        # \u89e3\u7801\u5668\u90e8\u5206\n        # \u7b2c\u4e00\u5c42\u89e3\u7801\u5668\n        self.conv0_1 = VGGBlock(self.nb_filter[0]+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])\n        # \u7b2c\u4e8c\u5c42\u89e3\u7801\u5668\n        self.conv1_1 = VGGBlock(self.nb_filter[1]+self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])\n        # \u7b2c\u4e09\u5c42\u89e3\u7801\u5668\n        self.conv2_1 = VGGBlock(self.nb_filter[2]+self.nb_filter[3], self.nb_filter[2], self.nb_filter[2])\n        # \u7b2c\u56db\u5c42\u89e3\u7801\u5668\n        self.conv3_1 = VGGBlock(self.nb_filter[3]+self.nb_filter[4], self.nb_filter[3], self.nb_filter[3])\n        # \u7b2c\u56db\u5c42\u89e3\u7801\u5668\u7684KAN\u5757\n        self.dblock3 = KANBlock(dim=self.nb_filter[3], norm_layer=nn.LayerNorm)\n        # \u5d4c\u5957\u8fde\u63a5\u7684\u89e3\u7801\u5668\n        self.conv0_2 = VGGBlock(self.nb_filter[0]*2+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])\n        self.conv1_2 = VGGBlock(self.nb_filter[1]*2+self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])\n        self.conv2_2 = VGGBlock(self.nb_filter[2]*2+self.nb_filter[3], self.nb_filter[2], self.nb_filter[2])\n        # \u7b2c\u4e09\u5c42\u89e3\u7801\u5668\u7684KAN\u5757\n        self.dblock2 = KANBlock(dim=self.nb_filter[2], norm_layer=nn.LayerNorm)\n        # \u66f4\u6df1\u5c42\u7684\u5d4c\u5957\u8fde\u63a5\n        self.conv0_3 = VGGBlock(self.nb_filter[0]*3+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])\n        self.conv1_3 = VGGBlock(self.nb_filter[1]*3+self.nb_filter[2], self.nb_filter[1], self.nb_filter[1])\n        self.conv0_4 = VGGBlock(self.nb_filter[0]*4+self.nb_filter[1], self.nb_filter[0], self.nb_filter[0])\n\n        # \u4e3ax0_1, x0_2, x0_3, x0_4\u7279\u5f81\u56fe\u521b\u5efaCBAM\u6a21\u5757\n        self.cbam_x0_1 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)\n        self.cbam_x0_2 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)\n        self.cbam_x0_3 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)\n        self.cbam_x0_4 = CBAMBlock(channel=self.nb_filter[0], reduction=16, kernel_size=3)\n\n        # \u8f93\u51fa\u5c42\n        if self.deep_supervision:\n            # \u5982\u679c\u4f7f\u7528\u6df1\u5ea6\u76d1\u7763\uff0c\u4e3a\u6bcf\u4e2a\u89e3\u7801\u5668\u8f93\u51fa\u521b\u5efa\u4e00\u4e2a\u5377\u79ef\u5c42\n            self.final1 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)\n            self.final2 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)\n            self.final3 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)\n            self.final4 = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)\n        else:\n            # \u5426\u5219\u53ea\u4e3a\u6700\u7ec8\u8f93\u51fa\u521b\u5efa\u4e00\u4e2a\u5377\u79ef\u5c42\n            self.final = nn.Conv2d(self.nb_filter[0], num_classes, kernel_size=1)\n\n    def forward(self, input):\n        '''\n        \u524d\u5411\u4f20\u64ad\u51fd\u6570\n        \u5b9e\u73b0UNet++\u7684\u524d\u5411\u4f20\u64ad\uff0c\u5e76\u5728\u6df1\u5c42\u7279\u5f81\u4e2d\u5e94\u7528KAN\u6a21\u5757\n        '''\n        # \u7f16\u7801\u5668\u8def\u5f84\n        # \u7b2c\u4e00\u5c42\u7279\u5f81\n        x0_0 = self.conv0_0(input)\n        # \u7b2c\u4e8c\u5c42\u7279\u5f81\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        # \u7b2c\u4e00\u5c42\u89e3\u7801\u5668\u7279\u5f81\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n        # \u5e94\u7528CBAM\u5230x0_1\n        x0_1 = self.cbam_x0_1(x0_1)\n\n        # \u7b2c\u4e09\u5c42\u7279\u5f81\uff08\u7ecf\u8fc7KAN\u5904\u7406\uff09\n        x2_0_raw = self.conv2_0(self.pool(x1_0))\n        # \u65b0\u589e\uff1a\u5c06x2_0\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\uff0c\u5e94\u7528KAN\u5757\n        out, H, W = self.patch_embed2(x2_0_raw)\n        out = self.block2(out, H, W)\n        # \u5c06\u5e8f\u5217\u8868\u793a\u8f6c\u6362\u56de\u7a7a\u95f4\u8868\u793a\n        x2_0 = out.reshape(-1, H, W, self.nb_filter[2]).permute(0, 3, 1, 2).contiguous()\n        \n        # \u7b2c\u4e8c\u5c42\u89e3\u7801\u5668\u7279\u5f81\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n        # \u7b2c\u4e00\u5c42\u6df1\u5ea6\u89e3\u7801\u5668\u7279\u5f81\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n        # \u5e94\u7528CBAM\u5230x0_2\n        x0_2 = self.cbam_x0_2(x0_2)\n\n        # \u7b2c\u56db\u5c42\u7279\u5f81\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        # \u5c06x3_0\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\uff0c\u5e94\u7528KAN\u5757\n        out, H, W = self.patch_embed3(x3_0)\n        out = self.block3(out, H, W)\n        # \u5c06\u5e8f\u5217\u8868\u793a\u8f6c\u6362\u56de\u7a7a\u95f4\u8868\u793a\n        x3_0 = out.reshape(-1, H, W, self.nb_filter[3]).permute(0, 3, 1, 2).contiguous()\n\n        # \u7b2c\u4e94\u5c42\u7279\u5f81\n        # \u5c06x3_0\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\uff0c\u5e94\u7528KAN\u5757\n        out, H, W = self.patch_embed4(x3_0)\n        out = self.block4(out, H, W)\n        # \u5c06\u5e8f\u5217\u8868\u793a\u8f6c\u6362\u56de\u7a7a\u95f4\u8868\u793a\n        x4_0 = out.reshape(-1, H, W, self.nb_filter[4]).permute(0, 3, 1, 2).contiguous()\n\n        # \u7b2c\u56db\u5c42\u89e3\u7801\u5668\u7279\u5f81\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n        # \u5c06x3_1\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\uff0c\u5e94\u7528KAN\u5757\n        _, _, H, W = x3_1.shape\n        out = x3_1.flatten(2).transpose(1, 2)\n        out = self.dblock3(out, H, W)\n        # \u5c06\u5e8f\u5217\u8868\u793a\u8f6c\u6362\u56de\u7a7a\u95f4\u8868\u793a\n        x3_1 = out.reshape(-1, H, W, self.nb_filter[3]).permute(0, 3, 1, 2).contiguous()\n\n        # \u4fee\u590d\u5c3a\u5bf8\u4e0d\u5339\u914d\u95ee\u9898\uff1a\u5bf9x3_1\u8fdb\u884c\u4e24\u6b21\u4e0a\u91c7\u6837\n        # \u4ece14x14\u4e0a\u91c7\u6837\u523028x28\uff0c\u518d\u523056x56\n        x3_1_up = self.up(self.up(x3_1))\n        # \u7b2c\u4e09\u5c42\u89e3\u7801\u5668\u7279\u5f81\n        x2_1 = self.conv2_1(torch.cat([x2_0, x3_1_up], 1))\n        \n        # \u7b2c\u4e8c\u5c42\u6df1\u5ea6\u89e3\u7801\u5668\u7279\u5f81\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n        # \u7b2c\u4e00\u5c42\u66f4\u6df1\u89e3\u7801\u5668\u7279\u5f81\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n        # \u5e94\u7528CBAM\u5230x0_3\n        x0_3 = self.cbam_x0_3(x0_3)\n\n        # \u7b2c\u4e09\u5c42\u6df1\u5ea6\u89e3\u7801\u5668\u7279\u5f81\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, x3_1_up], 1))  # \u4f7f\u7528\u76f8\u540c\u7684\u4e0a\u91c7\u6837\u7ed3\u679c\n        # \u5c06x2_2\u8f6c\u6362\u4e3a\u5e8f\u5217\u8868\u793a\uff0c\u5e94\u7528KAN\u5757\n        _, _, H, W = x2_2.shape\n        out = x2_2.flatten(2).transpose(1, 2)\n        out = self.dblock2(out, H, W)\n        # \u5c06\u5e8f\u5217\u8868\u793a\u8f6c\u6362\u56de\u7a7a\u95f4\u8868\u793a\n        x2_2 = out.reshape(-1, H, W, self.nb_filter[2]).permute(0, 3, 1, 2).contiguous()\n\n        # \u7b2c\u4e8c\u5c42\u66f4\u6df1\u89e3\u7801\u5668\u7279\u5f81\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n        # \u6700\u7ec8\u89e3\u7801\u5668\u7279\u5f81\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n        # \u5e94\u7528CBAM\u5230x0_4\n        x0_4 = self.cbam_x0_4(x0_4)\n\n        # \u8f93\u51fa\u5904\u7406\n        if self.deep_supervision:\n            # \u5982\u679c\u4f7f\u7528\u6df1\u5ea6\u76d1\u7763\uff0c\u8fd4\u56de\u6240\u6709\u89e3\u7801\u5668\u7684\u8f93\u51fa\n            output1 = self.final1(x0_1)\n            output2 = self.final2(x0_2)\n            output3 = self.final3(x0_3)\n            output4 = self.final4(x0_4)\n            return [output1, output2, output3, output4]\n        else:\n            # \u5426\u5219\u53ea\u8fd4\u56de\u6700\u7ec8\u8f93\u51fa\n            output = self.final(x0_4)\n            return output \n```\n\n\nkan.py\n```\nimport torch\nimport torch.nn.functional as F\nimport math\n\n\nclass KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer('grid', grid)\n\n        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = torch.nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        '''\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        '''\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        '''\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        '''\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        return base_output + spline_output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.concatenate(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        '''\n        Compute the regularization loss.\n\n        This is a dumb simulation of the original L1 regularization as stated in the\n        paper, since the original one requires computing absolutes and entropy from the\n        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n        behind the F.linear function if we want an memory efficient implementation.\n\n        The L1 regularization is now computed as mean absolute value of the spline\n        weights. The authors implementation also includes this term in addition to the\n        sample-based regularization.\n        '''\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n\n\nclass KAN(torch.nn.Module):\n    def __init__(\n        self,\n        layers_hidden,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KAN, self).__init__()\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        self.layers = torch.nn.ModuleList()\n        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n            self.layers.append(\n                KANLinear(\n                    in_features,\n                    out_features,\n                    grid_size=grid_size,\n                    spline_order=spline_order,\n                    scale_noise=scale_noise,\n                    scale_base=scale_base,\n                    scale_spline=scale_spline,\n                    base_activation=base_activation,\n                    grid_eps=grid_eps,\n                    grid_range=grid_range,\n                )\n            )\n\n    def forward(self, x: torch.Tensor, update_grid=False):\n        for layer in self.layers:\n            if update_grid:\n                layer.update_grid(x)\n            x = layer(x)\n        return x\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        return sum(\n            layer.regularization_loss(regularize_activation, regularize_entropy)\n            for layer in self.layers\n        )\n```\n\n\ndataset.py\n```\nimport os\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, num_classes, transform=None):\n        '''\n        Args:\n            img_ids (list): Image ids.\n            img_dir: Image file directory.\n            mask_dir: Mask file directory.\n            img_ext (str): Image file extension.\n            mask_ext (str): Mask file extension.\n            num_classes (int): Number of classes.\n            transform (Compose, optional): Compose transforms of albumentations. Defaults to None.\n        \n        Note:\n            Make sure to put the files as the following structure:\n            <dataset name>\n            \u251c\u2500\u2500 images\n            |   \u251c\u2500\u2500 0a7e06.jpg\n            \u2502   \u251c\u2500\u2500 0aab0a.jpg\n            \u2502   \u251c\u2500\u2500 0b1761.jpg\n            \u2502   \u251c\u2500\u2500 ...\n            |\n            \u2514\u2500\u2500 masks\n                \u251c\u2500\u2500 0\n                |   \u251c\u2500\u2500 0a7e06.png\n                |   \u251c\u2500\u2500 0aab0a.png\n                |   \u251c\u2500\u2500 0b1761.png\n                |   \u251c\u2500\u2500 ...\n                |\n                \u251c\u2500\u2500 1\n                |   \u251c\u2500\u2500 0a7e06.png\n                |   \u251c\u2500\u2500 0aab0a.png\n                |   \u251c\u2500\u2500 0b1761.png\n                |   \u251c\u2500\u2500 ...\n                ...\n        '''\n        self.img_ids = img_ids\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_ext = img_ext\n        self.mask_ext = mask_ext\n        self.num_classes = num_classes\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        \n        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n\n        mask = []\n        for i in range(self.num_classes):\n            mask.append(cv2.imread(os.path.join(self.mask_dir, str(i),\n                        img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n        #\u6570\u7ec4\u6cbf\u6df1\u5ea6\u65b9\u5411\u8fdb\u884c\u62fc\u63a5\u3002", "top": 0, "createdAt": 1766021330, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-12-18", "dateLabelColor": "#0969da"}, "P27": {"htmlDir": "docs/post/unet++.html", "labels": ["course"], "postTitle": "unet++", "postUrl": "post/unet%2B%2B.html", "postSourceUrl": "https://github.com/Aloner63/Aloner63.github.io/issues/27", "commentNum": 0, "wordCount": 27976, "description": "train.py\n```\nimport argparse\nimport os\nfrom collections import OrderedDict\nfrom glob import glob\n\nimport pandas as pd\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim as optim\nimport yaml\nfrom albumentations.augmentations import transforms\nfrom albumentations.core.composition import Compose, OneOf\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim import lr_scheduler\nfrom tqdm import tqdm\n\nimport archs\nimport losses\nfrom dataset import Dataset\nfrom metrics import iou_score\nfrom utils import AverageMeter, str2bool\n\nARCH_NAMES = archs.__all__\nLOSS_NAMES = losses.__all__\nLOSS_NAMES.append('BCEWithLogitsLoss')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--name', default=None,\n                        help='model name: (default: arch+timestamp)')\n    parser.add_argument('--epochs', default=100, type=int, metavar='N',\n                        help='number of total epochs to run')\n    parser.add_argument('-b', '--batch_size', default=16, type=int,\n                        metavar='N', help='mini-batch size (default: 16)')\n    \n    # model\n    parser.add_argument('--arch', '-a', metavar='ARCH', default='NestedUNet',\n                        choices=ARCH_NAMES,\n                        help='model architecture: ' +\n                        ' | '.join(ARCH_NAMES) +\n                        ' (default: NestedUNet)')\n    parser.add_argument('--deep_supervision', default=False, type=str2bool)\n    parser.add_argument('--input_channels', default=3, type=int,\n                        help='input channels')\n    parser.add_argument('--num_classes', default=1, type=int,\n                        help='number of classes')\n    parser.add_argument('--input_w', default=96, type=int,\n                        help='image width')\n    parser.add_argument('--input_h', default=96, type=int,\n                        help='image height')\n    \n    # loss\n    parser.add_argument('--loss', default='BCEDiceLoss',\n                        choices=LOSS_NAMES,\n                        help='loss: ' +\n                        ' | '.join(LOSS_NAMES) +\n                        ' (default: BCEDiceLoss)')\n    \n    # dataset\n    parser.add_argument('--dataset', default='dsb2018_96',\n                        help='dataset name')\n    parser.add_argument('--img_ext', default='.png',\n                        help='image file extension')\n    parser.add_argument('--mask_ext', default='.png',\n                        help='mask file extension')\n\n    # optimizer\n    parser.add_argument('--optimizer', default='SGD',\n                        choices=['Adam', 'SGD'],\n                        help='loss: ' +\n                        ' | '.join(['Adam', 'SGD']) +\n                        ' (default: Adam)')\n    parser.add_argument('--lr', '--learning_rate', default=1e-3, type=float,\n                        metavar='LR', help='initial learning rate')\n    parser.add_argument('--momentum', default=0.9, type=float,\n                        help='momentum')\n    parser.add_argument('--weight_decay', default=1e-4, type=float,\n                        help='weight decay')\n    parser.add_argument('--nesterov', default=False, type=str2bool,\n                        help='nesterov')\n\n    # scheduler\n    parser.add_argument('--scheduler', default='CosineAnnealingLR',\n                        choices=['CosineAnnealingLR', 'ReduceLROnPlateau', 'MultiStepLR', 'ConstantLR'])\n    parser.add_argument('--min_lr', default=1e-5, type=float,\n                        help='minimum learning rate')\n    parser.add_argument('--factor', default=0.1, type=float)\n    parser.add_argument('--patience', default=2, type=int)\n    parser.add_argument('--milestones', default='1,2', type=str)\n    parser.add_argument('--gamma', default=2/3, type=float)\n    parser.add_argument('--early_stopping', default=-1, type=int,\n                        metavar='N', help='early stopping (default: -1)')\n    \n    parser.add_argument('--num_workers', default=4, type=int)\n\n    config = parser.parse_args()\n\n    return config\n\n\ndef train(config, train_loader, model, criterion, optimizer):\n    avg_meters = {'loss': AverageMeter(),\n                  'iou': AverageMeter()}\n\n    model.train()\n\n    pbar = tqdm(total=len(train_loader))\n    for input, target, _ in train_loader:\n        input = input.cuda()\n        target = target.cuda()\n\n        # compute output\n        if config['deep_supervision']:\n            outputs = model(input)\n            loss = 0\n            for output in outputs:\n                loss += criterion(output, target)\n            loss /= len(outputs)\n            iou = iou_score(outputs[-1], target)\n        else:\n            output = model(input)\n            loss = criterion(output, target)\n            iou = iou_score(output, target)\n\n        # compute gradient and do optimizing step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        avg_meters['loss'].update(loss.item(), input.size(0))\n        avg_meters['iou'].update(iou, input.size(0))\n\n        postfix = OrderedDict([\n            ('loss', avg_meters['loss'].avg),\n            ('iou', avg_meters['iou'].avg),\n        ])\n        pbar.set_postfix(postfix)\n        pbar.update(1)\n    pbar.close()\n\n    return OrderedDict([('loss', avg_meters['loss'].avg),\n                        ('iou', avg_meters['iou'].avg)])\n\n\ndef validate(config, val_loader, model, criterion):\n    avg_meters = {'loss': AverageMeter(),\n                  'iou': AverageMeter()}\n\n    # switch to evaluate mode\n    model.eval()\n\n    with torch.no_grad():\n        pbar = tqdm(total=len(val_loader))\n        for input, target, _ in val_loader:\n            input = input.cuda()\n            target = target.cuda()\n\n            # compute output\n            if config['deep_supervision']:\n                outputs = model(input)\n                loss = 0\n                for output in outputs:\n                    loss += criterion(output, target)\n                loss /= len(outputs)\n                iou = iou_score(outputs[-1], target)\n            else:\n                output = model(input)\n                loss = criterion(output, target)\n                iou = iou_score(output, target)\n\n            avg_meters['loss'].update(loss.item(), input.size(0))\n            avg_meters['iou'].update(iou, input.size(0))\n\n            postfix = OrderedDict([\n                ('loss', avg_meters['loss'].avg),\n                ('iou', avg_meters['iou'].avg),\n            ])\n            pbar.set_postfix(postfix)\n            pbar.update(1)\n        pbar.close()\n\n    return OrderedDict([('loss', avg_meters['loss'].avg),\n                        ('iou', avg_meters['iou'].avg)])\n\n\ndef main():\n    config = vars(parse_args())\n\n    if config['name'] is None:\n        if config['deep_supervision']:\n            config['name'] = '%s_%s_wDS' % (config['dataset'], config['arch'])\n        else:\n            config['name'] = '%s_%s_woDS' % (config['dataset'], config['arch'])\n    os.makedirs('models/%s' % config['name'], exist_ok=True)\n\n    print('-' * 20)\n    for key in config:\n        print('%s: %s' % (key, config[key]))\n    print('-' * 20)\n\n    with open('models/%s/config.yml' % config['name'], 'w') as f:\n        yaml.dump(config, f)\n\n    # define loss function (criterion)\n    if config['loss'] == 'BCEWithLogitsLoss':\n        criterion = nn.BCEWithLogitsLoss().cuda()\n    else:\n        criterion = losses.__dict__[config['loss']]().cuda()\n\n    cudnn.benchmark = True\n\n    # create model\n    print('=> creating model %s' % config['arch'])\n    model = archs.__dict__[config['arch']](config['num_classes'],\n                                           config['input_channels'],\n                                           config['deep_supervision'])\n\n    model = model.cuda()\n\n    params = filter(lambda p: p.requires_grad, model.parameters())\n    if config['optimizer'] == 'Adam':\n        optimizer = optim.Adam(\n            params, lr=config['lr'], weight_decay=config['weight_decay'])\n    elif config['optimizer'] == 'SGD':\n        optimizer = optim.SGD(params, lr=config['lr'], momentum=config['momentum'],\n                              nesterov=config['nesterov'], weight_decay=config['weight_decay'])\n    else:\n        raise NotImplementedError\n\n    if config['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=config['epochs'], eta_min=config['min_lr'])\n    elif config['scheduler'] == 'ReduceLROnPlateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=config['factor'], patience=config['patience'],\n                                                   verbose=1, min_lr=config['min_lr'])\n    elif config['scheduler'] == 'MultiStepLR':\n        scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[int(e) for e in config['milestones'].split(',')], gamma=config['gamma'])\n    elif config['scheduler'] == 'ConstantLR':\n        scheduler = None\n    else:\n        raise NotImplementedError\n\n    # Data loading code\n    img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))\n    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n\n    train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n\n    train_transform = Compose([\n        transforms.RandomRotate90(),\n        transforms.Flip(),\n        OneOf([\n            transforms.HueSaturationValue(),\n            transforms.RandomBrightness(),\n            transforms.RandomContrast(),\n        ], p=1),\n        transforms.Resize(config['input_h'], config['input_w']),\n        transforms.Normalize(),\n    ])\n\n    val_transform = Compose([\n        transforms.Resize(config['input_h'], config['input_w']),\n        transforms.Normalize(),\n    ])\n\n    train_dataset = Dataset(\n        img_ids=train_img_ids,\n        img_dir=os.path.join('inputs', config['dataset'], 'images'),\n        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=train_transform)\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join('inputs', config['dataset'], 'images'),\n        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=val_transform)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        drop_last=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        drop_last=False)\n\n    log = OrderedDict([\n        ('epoch', []),\n        ('lr', []),\n        ('loss', []),\n        ('iou', []),\n        ('val_loss', []),\n        ('val_iou', []),\n    ])\n\n    best_iou = 0\n    trigger = 0\n    for epoch in range(config['epochs']):\n        print('Epoch [%d/%d]' % (epoch, config['epochs']))\n\n        # train for one epoch\n        train_log = train(config, train_loader, model, criterion, optimizer)\n        # evaluate on validation set\n        val_log = validate(config, val_loader, model, criterion)\n\n        if config['scheduler'] == 'CosineAnnealingLR':\n            scheduler.step()\n        elif config['scheduler'] == 'ReduceLROnPlateau':\n            scheduler.step(val_log['loss'])\n\n        print('loss %.4f - iou %.4f - val_loss %.4f - val_iou %.4f'\n              % (train_log['loss'], train_log['iou'], val_log['loss'], val_log['iou']))\n\n        log['epoch'].append(epoch)\n        log['lr'].append(config['lr'])\n        log['loss'].append(train_log['loss'])\n        log['iou'].append(train_log['iou'])\n        log['val_loss'].append(val_log['loss'])\n        log['val_iou'].append(val_log['iou'])\n\n        pd.DataFrame(log).to_csv('models/%s/log.csv' %\n                                 config['name'], index=False)\n\n        trigger += 1\n\n        if val_log['iou'] > best_iou:\n            torch.save(model.state_dict(), 'models/%s/model.pth' %\n                       config['name'])\n            best_iou = val_log['iou']\n            print('=> saved best model')\n            trigger = 0\n\n        # early stopping\n        if config['early_stopping'] >= 0 and trigger >= config['early_stopping']:\n            print('=> early stopping')\n            break\n\n        torch.cuda.empty_cache()\n\n\nif __name__ == '__main__':\n    main()\n\n```\n\n\narchs.py\n```\nimport torch\nfrom torch import nn\n\n__all__ = ['UNet', 'NestedUNet']\n\n\nclass VGGBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_channels, middle_channels, 3, padding=1)\n        self.bn1 = nn.BatchNorm2d(middle_channels)\n        self.conv2 = nn.Conv2d(middle_channels, out_channels, 3, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass UNet(nn.Module):\n    def __init__(self, num_classes, input_channels=3, **kwargs):\n        super().__init__()\n\n        nb_filter = [32, 64, 128, 256, 512]\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n\n        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n        self.conv2_2 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n        self.conv1_3 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv0_4 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n\n        self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n\n\n    def forward(self, input):\n        x0_0 = self.conv0_0(input)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x4_0 = self.conv4_0(self.pool(x3_0))\n\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, self.up(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, self.up(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0, self.up(x1_3)], 1))\n\n        output = self.final(x0_4)\n        return output\n\n\nclass NestedUNet(nn.Module):\n    def __init__(self, num_classes, input_channels=3, deep_supervision=False, **kwargs):\n        super().__init__()\n\n        nb_filter = [32, 64, 128, 256, 512]\n\n        self.deep_supervision = deep_supervision\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n\n        self.conv0_0 = VGGBlock(input_channels, nb_filter[0], nb_filter[0])\n        self.conv1_0 = VGGBlock(nb_filter[0], nb_filter[1], nb_filter[1])\n        self.conv2_0 = VGGBlock(nb_filter[1], nb_filter[2], nb_filter[2])\n        self.conv3_0 = VGGBlock(nb_filter[2], nb_filter[3], nb_filter[3])\n        self.conv4_0 = VGGBlock(nb_filter[3], nb_filter[4], nb_filter[4])\n\n        self.conv0_1 = VGGBlock(nb_filter[0]+nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_1 = VGGBlock(nb_filter[1]+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv2_1 = VGGBlock(nb_filter[2]+nb_filter[3], nb_filter[2], nb_filter[2])\n        self.conv3_1 = VGGBlock(nb_filter[3]+nb_filter[4], nb_filter[3], nb_filter[3])\n\n        self.conv0_2 = VGGBlock(nb_filter[0]*2+nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_2 = VGGBlock(nb_filter[1]*2+nb_filter[2], nb_filter[1], nb_filter[1])\n        self.conv2_2 = VGGBlock(nb_filter[2]*2+nb_filter[3], nb_filter[2], nb_filter[2])\n\n        self.conv0_3 = VGGBlock(nb_filter[0]*3+nb_filter[1], nb_filter[0], nb_filter[0])\n        self.conv1_3 = VGGBlock(nb_filter[1]*3+nb_filter[2], nb_filter[1], nb_filter[1])\n\n        self.conv0_4 = VGGBlock(nb_filter[0]*4+nb_filter[1], nb_filter[0], nb_filter[0])\n\n        if self.deep_supervision:\n            self.final1 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n            self.final2 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n            self.final3 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n            self.final4 = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n        else:\n            self.final = nn.Conv2d(nb_filter[0], num_classes, kernel_size=1)\n\n\n    def forward(self, input):\n        x0_0 = self.conv0_0(input)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], 1))\n\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], 1))\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], 1))\n\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], 1))\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], 1))\n\n        x4_0 = self.conv4_0(self.pool(x3_0))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n\n        if self.deep_supervision:\n            output1 = self.final1(x0_1)\n            output2 = self.final2(x0_2)\n            output3 = self.final3(x0_3)\n            output4 = self.final4(x0_4)\n            return [output1, output2, output3, output4]\n\n        else:\n            output = self.final(x0_4)\n            return output\n\n```\n\nval.py\n```\nimport argparse\nimport os\nfrom glob import glob\n\nimport cv2\nimport torch\nimport torch.backends.cudnn as cudnn\nimport yaml\nfrom albumentations.augmentations import transforms\nfrom albumentations.core.composition import Compose\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nimport archs\nfrom dataset import Dataset\nfrom metrics import iou_score\nfrom utils import AverageMeter\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--name', default=None,\n                        help='model name')\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    with open('models/%s/config.yml' % args.name, 'r') as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n    print('-'*20)\n    for key in config.keys():\n        print('%s: %s' % (key, str(config[key])))\n    print('-'*20)\n\n    cudnn.benchmark = True\n\n    # create model\n    print('=> creating model %s' % config['arch'])\n    model = archs.__dict__[config['arch']](config['num_classes'],\n                                           config['input_channels'],\n                                           config['deep_supervision'])\n\n    model = model.cuda()\n\n    # Data loading code\n    img_ids = glob(os.path.join('inputs', config['dataset'], 'images', '*' + config['img_ext']))\n    img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n\n    _, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=41)\n\n    model.load_state_dict(torch.load('models/%s/model.pth' %\n                                     config['name']))\n    model.eval()\n\n    val_transform = Compose([\n        transforms.Resize(config['input_h'], config['input_w']),\n        transforms.Normalize(),\n    ])\n\n    val_dataset = Dataset(\n        img_ids=val_img_ids,\n        img_dir=os.path.join('inputs', config['dataset'], 'images'),\n        mask_dir=os.path.join('inputs', config['dataset'], 'masks'),\n        img_ext=config['img_ext'],\n        mask_ext=config['mask_ext'],\n        num_classes=config['num_classes'],\n        transform=val_transform)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        drop_last=False)\n\n    avg_meter = AverageMeter()\n\n    for c in range(config['num_classes']):\n        os.makedirs(os.path.join('outputs', config['name'], str(c)), exist_ok=True)\n    with torch.no_grad():\n        for input, target, meta in tqdm(val_loader, total=len(val_loader)):\n            input = input.cuda()\n            target = target.cuda()\n\n            # compute output\n            if config['deep_supervision']:\n                output = model(input)[-1]\n            else:\n                output = model(input)\n\n            iou = iou_score(output, target)\n            avg_meter.update(iou, input.size(0))\n\n            output = torch.sigmoid(output).cpu().numpy()\n\n            for i in range(len(output)):\n                for c in range(config['num_classes']):\n                    cv2.imwrite(os.path.join('outputs', config['name'], str(c), meta['img_id'][i] + '.jpg'),\n                                (output[i, c] * 255).astype('uint8'))\n\n    print('IoU: %.4f' % avg_meter.avg)\n\n    torch.cuda.empty_cache()\n\n\nif __name__ == '__main__':\n    main()\n\n```\n\nmetrics.py\n```\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\ndef iou_score(output, target):\n    smooth = 1e-5\n\n    if torch.is_tensor(output):\n        output = torch.sigmoid(output).data.cpu().numpy()\n    if torch.is_tensor(target):\n        target = target.data.cpu().numpy()\n    output_ = output > 0.5\n    target_ = target > 0.5\n    intersection = (output_ & target_).sum()\n    union = (output_ | target_).sum()\n\n    return (intersection + smooth) / (union + smooth)\n\n\ndef dice_coef(output, target):\n    smooth = 1e-5\n\n    output = torch.sigmoid(output).view(-1).data.cpu().numpy()\n    target = target.view(-1).data.cpu().numpy()\n    intersection = (output * target).sum()\n\n    return (2. * intersection + smooth) / \\\n        (output.sum() + target.sum() + smooth)\n\n```\nutils.py\n```\nimport argparse\n\n\ndef str2bool(v):\n    if v.lower() in ['true', 1]:\n        return True\n    elif v.lower() in ['false', 0]:\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\n\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nclass AverageMeter(object):\n    '''Computes and stores the average and current value'''\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n```\n\nlosses.py\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from LovaszSoftmax.pytorch.lovasz_losses import lovasz_hinge\nexcept ImportError:\n    pass\n\n__all__ = ['BCEDiceLoss', 'LovaszHingeLoss']\n\n\nclass BCEDiceLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input, target):\n        bce = F.binary_cross_entropy_with_logits(input, target)\n        smooth = 1e-5\n        input = torch.sigmoid(input)\n        num = target.size(0)\n        input = input.view(num, -1)\n        target = target.view(num, -1)\n        intersection = (input * target)\n        dice = (2. * intersection.sum(1) + smooth) / (input.sum(1) + target.sum(1) + smooth)\n        dice = 1 - dice.sum() / num\n        return 0.5 * bce + dice\n\n\nclass LovaszHingeLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input, target):\n        input = input.squeeze(1)\n        target = target.squeeze(1)\n        loss = lovasz_hinge(input, target, per_image=True)\n\n        return loss\n\n```\ndataset.py\n```\nimport os\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.utils.data\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, img_ids, img_dir, mask_dir, img_ext, mask_ext, num_classes, transform=None):\n        '''\n        Args:\n            img_ids (list): Image ids.\n            img_dir: Image file directory.\n            mask_dir: Mask file directory.\n            img_ext (str): Image file extension.\n            mask_ext (str): Mask file extension.\n            num_classes (int): Number of classes.\n            transform (Compose, optional): Compose transforms of albumentations. Defaults to None.\n        \n        Note:\n            Make sure to put the files as the following structure:\n            <dataset name>\n            \u251c\u2500\u2500 images\n            |   \u251c\u2500\u2500 0a7e06.jpg\n            \u2502   \u251c\u2500\u2500 0aab0a.jpg\n            \u2502   \u251c\u2500\u2500 0b1761.jpg\n            \u2502   \u251c\u2500\u2500 ...\n            |\n            \u2514\u2500\u2500 masks\n                \u251c\u2500\u2500 0\n                |   \u251c\u2500\u2500 0a7e06.png\n                |   \u251c\u2500\u2500 0aab0a.png\n                |   \u251c\u2500\u2500 0b1761.png\n                |   \u251c\u2500\u2500 ...\n                |\n                \u251c\u2500\u2500 1\n                |   \u251c\u2500\u2500 0a7e06.png\n                |   \u251c\u2500\u2500 0aab0a.png\n                |   \u251c\u2500\u2500 0b1761.png\n                |   \u251c\u2500\u2500 ...\n                ...\n        '''\n        self.img_ids = img_ids\n        self.img_dir = img_dir\n        self.mask_dir = mask_dir\n        self.img_ext = img_ext\n        self.mask_ext = mask_ext\n        self.num_classes = num_classes\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def __getitem__(self, idx):\n        img_id = self.img_ids[idx]\n        \n        img = cv2.imread(os.path.join(self.img_dir, img_id + self.img_ext))\n\n        mask = []\n        for i in range(self.num_classes):\n            mask.append(cv2.imread(os.path.join(self.mask_dir, str(i),\n                        img_id + self.mask_ext), cv2.IMREAD_GRAYSCALE)[..., None])\n        mask = np.dstack(mask)\n\n        if self.transform is not None:\n            augmented = self.transform(image=img, mask=mask)\n            img = augmented['image']\n            mask = augmented['mask']\n        \n        img = img.astype('float32') / 255\n        img = img.transpose(2, 0, 1)\n        mask = mask.astype('float32') / 255\n        mask = mask.transpose(2, 0, 1)\n        \n        return img, mask, {'img_id': img_id}\n\n```\n\npreprocess_dsb2018.py\n```\nimport os\nfrom glob import glob\n\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef main():\n    img_size = 96\n\n    paths = glob('inputs/data-science-bowl-2018/stage1_train/*')\n\n    os.makedirs('inputs/dsb2018_%d/images' % img_size, exist_ok=True)\n    os.makedirs('inputs/dsb2018_%d/masks/0' % img_size, exist_ok=True)\n\n    for i in tqdm(range(len(paths))):\n        path = paths[i]\n        img = cv2.imread(os.path.join(path, 'images',\n                         os.path.basename(path) + '.png'))\n        mask = np.zeros((img.shape[0], img.shape[1]))\n        for mask_path in glob(os.path.join(path, 'masks', '*')):\n            mask_ = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) > 127\n            mask[mask_] = 1\n        if len(img.shape) == 2:\n            img = np.tile(img[..., None], (1, 1, 3))\n        if img.shape[2] == 4:\n            img = img[..., :3]\n        img = cv2.resize(img, (img_size, img_size))\n        mask = cv2.resize(mask, (img_size, img_size))\n        cv2.imwrite(os.path.join('inputs/dsb2018_%d/images' % img_size,\n                    os.path.basename(path) + '.png'), img)\n        cv2.imwrite(os.path.join('inputs/dsb2018_%d/masks/0' % img_size,\n                    os.path.basename(path) + '.png'), (mask * 255).astype('uint8'))\n\n\nif __name__ == '__main__':\n    main()\n\n```\n\u3002", "top": 0, "createdAt": 1766747876, "style": "", "script": "", "head": "", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "createdDate": "2025-12-26", "dateLabelColor": "#0969da"}}, "singeListJson": {}, "labelColorDict": {"bug": "#d73a4a", "course": "#6376F9", "documentation": "#0075ca", "duplicate": "#cfd3d7", "enhancement": "#a2eeef", "good first issue": "#7057ff", "help wanted": "#008672", "invalid": "#e4e669", "problem": "#46DCA1", "question": "#d876e3", "wontfix": "#ffffff"}, "displayTitle": "Aloner63 \u7684\u4e2a\u4eba\u535a\u5ba2", "faviconUrl": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "ogImage": "https://raw.githubusercontent.com/Aloner63/mymm/typora/typora/1340761045.jpeg", "primerCSS": "<link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />", "homeUrl": "https://Aloner63.github.io", "prevUrl": "/index.html", "nextUrl": "disabled"}